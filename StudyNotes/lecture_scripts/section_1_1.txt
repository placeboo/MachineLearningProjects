CHAPTER 1 SUPERVISED LEARNING

1.1 Decision Trees


1.1.1 Difference between Classification and Regression
Okay, hi Michael. Hey Charles, how’s it going. It’s going pretty well, how’s it going in your end of the world? Very nice, what are we want to talk about today? Well today we are going to talk about supervised learning. But, in particular what we’re going to talk about are two kinds of supervised learning, and one particular way to do supervised learning. Okay, so the two types of supervised learning that we typically think about are classification. And regression. And we’re going to spend most of the time today talking about classification and more time next time talking about regression. So the difference between classifica- tion and regression is fairly simple for the purposes of this discussion. Classification is simply the process of taking some kind of input, let’s call it x. And I’m going to define these terms in a couple of minutes. And mapping it to some discrete label. Usually, for what we’re talking about, something like, true or false. So, what’s a good example of that? Imagine that I have a nice little picture of Michael. It looks just like me! It looks exactly like you. So I have a nice little picture here and I want to know whether this is a male. Or a female. So given an input like this I will map it to male or female. So what do you think, Michael? Do you think this is a male or a female? So you’re, you’re classifying me as male or female based on the picture of me and I would think you know, based on how I look I’m clearly male. Yes. In fact, manly male. So, this would be a classification from pictures to male. The alternative would be something like a picture to female, and I’m just going to take a completely stereotypical image of either a female or. I think it’s actually, that’s actually me when I let my hair go long. Right, so, so which points out that this can be pretty hard. But this is where we’re going to spend most of our time talking about it first as a classification task. So taking some kind of input, in this case pictures, and mapping it to some discrete number of labels, true or false, male or female, car versus cougar, anything that, that you might imagine thinking of. Car versus cougar? Yes. That, I guess that’s an important thing if you’re driving. You don’t want to run into any cougars or probably other cars either. Well you know, you’re sitting down and you’re trying to decide whether you should ride this thing that you see or not. And if its a cougar maybe you don’t want to and if it’s a car maybe you do. Excellent. Don’t drive a cougar. Don’t drive a cougar. That’s the first lesson in machine learning. Excellent. Okay, so that’s classification. We’ll return to regression in a little bit later during this conversation. But, just as a preview, regression is more about continuous value function. So, something like giving a bunch of points. I want to give in a new point. I want to map it to some real value. So we may pretend that these are examples of a line and so given a point here, I might say the output is right there. Okay, so that’s regression but we’ll talk about that in a moment. Right now, what I want to talk about is classification. Would an example of regression also be, for example, mapping the pictures of me to the length of my hair? Like a number that represents the length of my hair? Absolutely, for the purposes of, of the sort of things that we’re going to be worried about you can really think of the difference between classification and regression is the difference between mapping from some input to some small number of discrete values which might represent concepts. And regression is mapping from some input space to some real number. Potentially infinite number of real numbers. Cool, let’s do a, let’s do a quiz. Make sure we get this. Okay, I like that.

1.1.2 Classification or Regression Question
So we’ve talked about uh,classification and regression and gave a couple of examples of each. So now I want you to take a moment to make certain that you understand the difference because it’s a crucial difference for supervised learning. Here’s a very short little quiz. You have three questions here and we divided the world up into some input to some learning algorithm, whatever that learning algorithm is. The output that we’re expecting and then a box for you to tell us whether its classification or regression. So, the first question, the input is credit history, whatever that means, the number of loans that you have, how much money you make, how many times you’ve defaulted, how many times you’ve been late, the sort of things that make up credit history, and the output of the learning algorithm is rather you should lend money or not. So you’re a bank, and you’re trying to determine whether given a credit history, I should lend this person money, that’s question one. Is that classification, or is that regression? Question two you take as input a picture like the examples that we’ve given before. And the output of your learning system is going to be whether this person is of high school age, college age, or grad student age. The third question is very similar. The input is again a picture. And the output is, I guess, of the actual age of the person, 17, 24, 23 and a half, whatever. So take a moment and try to decide whether these are classification tasks or regression tasks.

1.1.3 Classification or Regression Solution
And so now let’s give the explanation for the quiz. Alright, so, let’s see what happened here. So, what you’re saying is in some cases the inputs are discrete or continuous or complicated. In other cases the outputs could be discrete or continuous or complicated. But I think what you were saying is, that what on, on, what matters to determine if something is classification or regression is whether the output is from a discrete small set or, or whether it’s some continuous quantity. Is that right? Right, that’s exactly right. The difference between a classification task or a regression task is not about the input, it’s about the output. If the output is continuous then it’s regression and if the output is small discrete set or discrete set, then it is a classification task. So, with that in mind, what do you think is the answer to the first one? So, lend money. If it was something like predicting a credit score, that seems like a more continuous quantity. But this is just a yes no, so that’s a discrete class, so I’m going to go with classification. That is, correct. It is classification and the short answer is, because it’s a binary task. True, false. Yes, no. Lend money or don’t lend money. Got it. So it’s a simple classification test. Okay, with that in mind, what about number two? Alright, so number two. It’s trying to judge something about where they fall on a scale, high school, college, or grad student. But all of, the system is being asked to do is put them into one of those three categories, and these categories are like classes, so it’s classification. That is also exactly right. Classification. We moved from binary to trinary in this case, but the important thing is that it’s discrete. So it doesn’t matter if it’s high school, college grad, professor, elementary school, any number of other ways we might decide where your status of matriculation is is a small discrete set. So, with that in mind, what about number three? Alright, so the input is the same in this case. And the output is kind of the same except there’s, well there’s certainly more categories because there’s more possible ages than just those three. But when you gave the example you did explicitly say that ages can be fractional like, you know, 22.3. So that definitely makes me think that it’s continuous, so it should be regression. Right, I think that is exactly the right thing, you have a continuous output. Now, I do want to point something out. That while the right answer is regression, a lot of people might have decided that the answer was classification. So, what’s an argument? If I told you in fact the answer was classification, what would be your argument for why that would be? Well, I guess. Can you think of one? Yeah, I guess. I mean, you know, if you think about ages as being discrete. You just say, you know, you can one or two or three or, you know, whatever up to 130, say. But there, but there’s just that, that set. There isn’t really, you know, usually we don’t talk about fractional ages. So, so it seems like you could think of it as, as, as a set of classes. Right. So let’s imagine. So, how old are people? Let’s imagine we only cared about years, so you’re either one or two or three or four or five. Or maybe you can be one and a half, and two and a half, and three and a half. But, whatever, it’s, it’s not all possible real number values. And we know that people don’t live beyond, say, 250. Well, in that case, you’ve got a very large discrete set but it’s still discrete. Doesn’t matter whether there’s two things in your set, three things in your set, or in this case 250 things in your set, it’s still discrete. So, whether it’s regression, or classification, depends upon exactly how you define your output and these things become important. I’m going to argue that in practice, if you were going to set up this problem, the easiest way to do it would be to think about it as a real number and you would predict something like 23.7. And so it’s going to end up being a regression task and we can might, maybe think about that a little bit more as we move along. So either answer would be acceptable depending upon what your explanation of exactly what the output was. Was. You buy that? That makes sense. Excellent. Okay. Alright, let’s move beyond the quiz and start thinking about exactly what it means to define a classification problem. And then later what it means to define a regression problem.

1.1.4 Classification Learning One
Okay so, classification learning. Before we get into the details of that, I want to define a couple of terms. Because we’re going to be throwing these terms out all throughout the lessons. And I want to make certain that we’re all on the same page and we mean the same thing. But we’re returning to this again and again and again. So, the first term I want to introduce is the notion of instances. So, instances, or another way of thinking about them is input. Those are vectors of valu, of attributes. That define whatever your input space is. So they can be pictures, and all the pixels that make up pictures like we’ve been doing so far before. They can be credit score examples like how much money I make, or how much money Michael makes. [LAUGH] How much money I wish I made, so on and so fourth. So whatever your input value is, whatever it is you’re using to describe the input, whether it’s pixels or its discrete values, those are your instances, the set of things that you’re looking at. So you have instances, that’s the set of inputs that you have. And then what we’re trying to find is some kind of function And that is the concept that we care about. And this function actually maps inputs to outputs, so it takes the instances, it maps them in this case to some kind of output such as true or false. This is the, the categories of things that we’re worried about. And for most of the conversation that we’re going to be having, we’re going to be thinking about binary classification, just true and false. But the truth is, this would work whether there were three outputs, as we did with high school, college, or grad school, or whether there were 250 as we were contemplating for ages. But the main thing here is the concept, is the function that we care about that’s going to map pictures, for example, to true or false. So okay, I get, I get the use of the word,” instance”, right?” Instance” is just, like, a single thing that’s out there. But I have an intuitive notion of what a concept is. How does that relate to this more formal notion? Like, can we connect this to, kind of, the intuitive notion of what a concept is? I guess so. So a concept, I don’t know. How would you want to put that? So a concept is something That, I mean were talking about is a notion of a function, so what it means formally is that you have some input, like a picture, and it immediately inputs maps anything in that input space to some particular defined output space, like true or false, or male or female, or credit, credit worthy or not. Intuitively a concept is an idea describes a set of things. OK, so we can talk about maleness or femaleness. We can talk about short and tall; we can talk about college students and grad students. And so the concept is the notion of what creditworthiness is, what a male is, what a college student is. Okay, I think I see that. So, so essentially if you want to think of tallness, the concept of tallness, one way to define it is to say, Well in general if you give me something I can tell you rather or not its [UNKNOWN] so it’s going to map those somethings to am I tall or not. True or false. Right. Exactly and so really when you think about any concept and we talk about this in generally [UNKNOWN] is effectively a way of saying is effectively a set of things. That are apart of that concept. So, you can have a concept of a car and if I gave you ”cars” you would say these things are in it and if I gave you ”non-cars” you would say they are not. And so a concept is, therefore by definition, a mapping between objects in a world and membership in a set, which makes it a function. Okay, that makes sense. Okay.

1.1.5 Classification Learning Two
So with that, with that notion of a concept as functions or as mappings from objects to membership in a set we have a particular target concept. And the only difference between a target concept and the general notion of concept is that the target concept is the thing we’re trying to find. It’s the actual answer. All right? So, a function that determines whether something is a car or not, or male or not, is the target concept. Now this is actually important, right, because we could say that. We have this notion in our head of things that are cars or things that are males, or thing, or people who are credit worthy but unless we actually have it written down somewhere we don’t know whether it’s right or wrong. So there is some target concept we’re trying to get of all the concepts that might map pictures or people to true and false. Okay, so if you trying to teach me what tallness is so you have this kind of concept in mind of these, these things are tall and these things are not tall. So you’re going to have to somehow convey that to me. So how are you going to teach me? So that’s exactly the, well that’s what comes up with the rest of these things that we’re defining here. Okay. So let me tell you what the next four things are then you can tell me whether that answers your question. Got it. How about that? OK, so we’ve got a notion of instances, input, we’ve got a notion of concepts. Which take input and maps into some kind of output. And we’ve got some target concepts, some specific function from particular idea that we’re trying to find, we’re trying to represent. But out of what? So that’s where the hypothesis comes in. And in fact I think it’s better to say hypothesis class. So that’s the set of all concepts that you’re willing to entertain. So it’s all the functions I’m willing to think about. So why wouldn’t it just be all possible functions? It could be all possible functions and that’s a perfectly reasonable hypothesis class. The problem with that is that if it is all possible functions it may be very, very hard for you to figure out which function is the right one given finite data. So when we actually go over decision trees next I think it will be kind of clear why you need to pick a specific hypothesis class. Okay. So let’s return to that in a little bit but it’s an excellent question. So, conceptually in the back of your head until we, we come to specific examples, you can think of hypothesis class as simply being all possible functions in the world. On the other hand, even so far just the classification learning, we already know that we’re restricting ourselves to a subset of all possible functions in the world, because all possible functions in the world includes things like x squared, and that’s regeression. And we’ve already said, we’re not doing regression. We’re doing classification. So already hypothesis classes all functions we care about and maybe it’s all classification functions. So we’ve already picked a subset. So we got all these incidences, got all these concepts, we want to find a, a particular concept and we’ve got this set of functions we’re willing to look at. So how are we going to determine what the right answer is. So if you try to answer Micheal’s question that we do that in machine learning is with a sample or another name for which I prefer is a training set.

1.1.6 Classification Learning Three
So what’s a training set? Well a training set is a set of all of our inputs, like pictures of people, paired with a label, which is the correct output. So in this case, yes, this person is credit worthy. [LAUGH] Versus another example. You can tell I’m credit worthy based on my curly hair. Purely on the hair. Versus someone who has no curly hair and therefore is obviously not credit worthy. And if you get bunches and bunches of examples of input and output pairs, that’s a training set. And that’s what’s going to be the basis for you figuring out what is the correct concept or function. I see. So instead of just telling me what tall means, you’re going to give me lots of examples of, this is tall, this is not tall, this is tall, this is tall, this is tall, this is not tall. And that’s the way that you’re explaining what the target concept is. Right. So if you want to think about this in the real world, it’s as if we’re walking down the street and I’m pointing out cars to you, and non-cars to you, rather than trying to give you a dictionary that defines exactly what a car is. And that is fundamentally inductive learning as we talked about before. Lots and lots of examples, lots of labels. Now I have to generalize beyond that. So, last few things that we we talk about, last two terms I want to introduce are candidate, and testing set. So what’s a candidate? Well a candidate is just simply the, a concept that you think might be the target concept. So, for example, I might have, right now, you already did this where you said, oh, okay I see, clearly I’m credit worthy because I have curly hair. So, you’ve effectively asserted a particular function that looks at, looks for curly hair, and says, if there’s curly hair there, the person’s credit worthy. Which is certainly how I think about it. And people who are not curly hair, or do not have curly hair are, in fact, not credit worthy. So, that’s your target concept. And so, then, the question is, given that you have a bunch of examples, and you have a particular candidate or a candidate concept, how do you know whether you are right or wrong? How do you know whether it’s a good candidate or not? And that’s where the testing set comes in. So a testing set looks just like a training set. So here our training set, we’ll have pictures and whether someone turns out to be credit worthy or not. And I will take your candidate concept and I’ll determine whether it does a good job or not, by looking at the testing set. So in this case, because you decided curly hair matters, I have drawn, I have given you two examples from a training set, both of which have curly hair, but only one of which is deemed credit worthy. Which means your target concept is probably not right. So to do that test I, guess you can go through all the pictures in the testing set, apply the candidate concept to see whether it says true or false, and then compare that to what the testing set actually says that answer is. Right, and that’ll give you an error. So, by the way, the true target, the true target concept is whether you smile or not. Oh. That does make somebody credit worthy. It does in my world. Or at least I, wish it did in my world. Okay. So, by the way an important point is that the training set and the testing set should not be the same. If you learn from your training set, and I test you only on your training set, then that’s considered cheating in the machine learning world. Because then you haven’t really shown the ability to generalize. So typically we want the training set to include lots of examples that you don’t, the testing set, I’m sorry, to include lots of examples that you don’t see in your training set. And that is proof that you’re able to generalize. I see. So that, and that makes intuitive sense, right? So, like, if, if you’re a teacher and you’re telling me, you give me a bunch of fact and then you test me exactly that bunch of facts, it doesn’t, I don’t have to have understood them. I just can regurgitate them back. If you really want to see if I got the right concept, you have to see whether or not I can apply that concept in new examples. Yes, which is exactly why our final exams are written the way that they are written. Because you can argue that I’ve learned something by doing memorization, but the truth is you haven’t. You’ve just memorized. Here you have to do generalization. As you remember from our last discussion, generalization is the whole point of machine learning.

1.1.7 Example 1 Dating
All right, so we’ve defined our terms, we, we know, what it takes to do, at least supervised learning. So now I want to do a specific algorithm and a specific representation, that allows us to solve the problem of going from instances to, actual concepts. So what we’re going to talk about next are decision trees. And I think the best way to introduce decision trees is through an example. So, here’s the very simple example I want you to think about for a while. You’re on a date with someone. And you come to a restaurant. And you’re going to try to decide whether to enter the restaurant or not. So, your, input, your instances are going to be features about the restaurant. And we’ll talk a little bit about what those features might be. And the output is whether you should enter or not. Okay, so it’s a very simple, straightforward problem but there are a lot of details here that we have to figure out. It’s a classification problem. It’s clearly a classification problem because the output is yes, we should enter or no, we should move on to the next restaurant. So in fact, it’s not just a classification problem, it’s those binary classification problems that I said that we’d almost exclusively be thinking about for the next few minutes. Okay. So, you understand the problem set up? Yes, though I’m not sure exactly what the pieces of the input are. Right, so thats actually the right next question to ask. We have to actually be specific now about a representation. Before I was drawing squiggly little lines and you could imagine what they were, but now since we’re really going to go through an example, we need to be clear about what is it mean to be standing in front of the restaurant. So, let’s try to figure out how we would represent that, how we would define that. We’re talking about, you’re standing in front of a restaurant or eatery because I can’t see the reliably small restaurant. And we’re going to try to figure out whether we’re going to go in or not. But, what do we have to describe our eatery? What do we have? What are our attributes? What are the instances actually made of? So, what in, or another way of putting it is, what are the features that we need to pay attention to that are going to help us to determine whether we should yes, enter into the restaurant. Or no, move on to the next restaurant. So, any ideas Michael? Sure. I guess there’s like the type of restaurant. Okay, Oh, is it tall or short, and is it a good credit risk? [LAUGH] Oh wait, no, no, no wait, I know. Like the Italian versus French, versus, you know, Vietnamese. So let’s call that the type. So it could be Italian, it could be French, it could be Thai, it could be American, there are American restaurants, right? Sure. Greek, it can be, Armenian. It can any kind of restaurant you want to. Okay, good. So that’s something that probably matters because maybe you don’t like Italian food or maybe you’re really in the mood for Thai. Sounds perfect. Okay anything else? Sure. How about whether it smells good? You know, I like cleanliness. Let’s let’s, or you know what, let’s, let’s be nice to our eateries and let’s say atmosphere. Mm. Right because if there’s, you know, no atmosphere, then it is going to be really hard to breathe. That’s exactly right. So is it fancy? Is it a hole-in-the-wall, which I’m going to spell HIW. Is it a hole-in-the-wall, umm, those sorts of things. The, you know? Casual, I guess, is another category. Casual. And so on, and so forth. You could imagine lots of things like that, but these things might matter to you and your date. Okay, so, we know the type of the restaurant that we have, we know whether it’s fancy, whether it’s casual, whether it’s a hole in the wall. Some of the best food I’ve ever had are in you know, well-known hole in the walls. Those sorts of things. Anything else you can think of? Sure, Sometimes, I might use something like looking inside and seeing whether there’s people in there and whether they look they’re having a good time. Right. So that’s an important thing. So let’s just say If it’s occupied. Now why might that matter in reality? Well it matters because if it’s completely full and you may have to wait for a very long time, you might not want to go in. On the other hand. If you’re looking at a restaurant you’ve never heard of, and there’s only two people in it, and it’s Friday at 7 p.m. Maybe that says something about something. Maybe you want it to be quiet. You know, those sorts of things might matter. Okay, so, we’ve got type, we’ve got atmosphere, we’ve got occupied. Anything else you can think of? And I have been out of the dating market for a while, but I guess it could imagine, I could imagine how hard I am trying to work to impress my date. perfect. So do you have a hot date or not? Or, this is someone who you really, really, really want to impress and so, maybe it matters then, it’s even more important whether it’s a fancy restaurant or a hole in the wall, or whether it’s French or whether it’s an American restaurant. That make sense? I think that makes sense. Notice, by the way, that the first two sets that we have have multiple possible categories here. So it could be Italian, French, Thai, American, so on and so forth. Atmosphere is something that can have many, many possible values, but the last two things that we talked about were all binary. Either it’s occupied or it’s not. Yes or no or, you have a hot date or you don’t. And I think we could go on like this for a long time but, let’s try to move on to maybe a couple of other features and then try to actually figure out how we may actually solve this.

1.1.8 Representation
Alright, so Michael. Last set of features that that’s come up with three or four, three or four more features and then move on. Sure. So come up with a couple. Alright, so I could, sometimes I’ll look at the menu that’s posted outside, and I’ll see if the, you know? How pricey it is. Okay, so cost. Right, so cost can be represented as discrete input. By the way, it could also be represented as an actual number. Right? We could say, look it’s cheap, it’s moderately expensive, it’s really expensive or you could have a number which is the average cost of an entry. And it doesn’t really matter for, for what we’re talking about now but just some way of representing cost. Okay. Just give me one or two more features but I want to give me some features that don’t have anything to do with the restraunt itself but might still be important. Hmm. because, by the way, hot date probably doesn’t have anything to do with the restaurant itself. So, even though we’ve been talking the features of the restaurant. We’ve actually been picking up, at least, one feature that doesn’t have anything to do directly with the restaurant itself. Not to. So, whether I’m hungry? I like that. Here’s another one. What’s the weather like. Is it raining outside? Which is a different sense of atmosphere. Right. because if it’s raining outside, maybe it’s not your, your favorite choice but you don’t want to walk anymore. Okay, so we have a ton of features here. We’ve gone through a few of them. Notice that some of the specifically have to do with the restaurant and some of them have to do with things that are external to the restaurant itself but you can imagine that they’re all important. Or possibly important to whether you should enter into the restaurant or not. Agreed? And there’s a bunch of features you could imagine coming up with that probably have nothing to do with whether you should enter into the restaurant or not. Like, how many cars are currently parked across the country. Probably doesn’t have an impact on whether you’re going to enter into a specific restaurant or not. Okay. So, we have a whole bunch of features and right now we’re sticking with features we think that might be relevant. And we’re going to use those to make some kind of decision. So, that gets us to decision trees. So, the first thing, that, that we want to do is, we want to separate out what a decision tree is from the algorithm that you might use to build the decision tree. So the first thing we want to think about is the fact that a decision tree has a specific representation. Then only after we understand that representation and go through an example, we’ll start thinking about an algorithm for finding or building a decision tree. Okay, are you with me? Yeah. Excellent. Okay, so a decision tree is a very simple thing. Well, you might be, might be surprised to know it’s a tree, the first part of it. And it looks kind of like this. So, what I’ve drawn for you is example. Sample generic, decision tree. And what you’ll see is three parts to it. The first thing you’ll see is a circle. These are called nodes, and these are in fact, decision nodes. So, what you do here, is you pick a particular attribute. [NOISE] And you ask a question about it. The answer to that question, which is its value for what the edges represent in your tree. Okay. So we have these nodes which represent attributes, and we have edges which represent value. So let’s be specific about what that means. So here’s a particular attribute we might pick for the top node here. Let’s call it hungry. That’s one of the features that Michael came up with. Am I hungry or not? And there’s only two possible answers for that. yes, I’m hungry, true, or false, I am not hungry. And each of these nodes represent some attribute. And the edges represent the answers for specific values. So it’s as if I’m making a bunch of decisions by asking a series of questions. Am I hungry? And if the answer is yes, I am hungry, then I go and I ask a different question. Like is it rainy outside? And maybe it is rainy and maybe it’s not rainy, and let’s say if it isn’t rainy then I want to make a decision, and so these square boxes here are the actual output. Okay so it’s hungry, so you’re hungry, yes, and it’s not raining, so what do you do? So, let’s just say you go in. True, I go in so, when it’s, I’m hungry and it’s not raining, I go in. And that, that true is answering a different question. It’s not in the nodes I guess. So, in the leaves, the t and f means something different. That’s right. It’s the out, that’s exactly right. The, the leaves, the little boxes, the leaves of your decision tree is your answer. What’s on the on the edges are the possible values that your attribute can take on. So, in fact, let’s try to, let’s make that clear by picking a different by picking another possible attribute. You could imagine that if I am not hungry, what’s going to matter a lot now is say, the type of restaurant, right. Which we said there were many, many types of restaurants. So, you know thai, I forget [CROSSTALK] Italian. Italian, and you know, something. French fries. And French Fries. So if I’m not hungry, then what matters a lot more is the type of restaurant, and so I’ll move down this path instead and start asking other questions. But ultimately, no matter how I, what this decision tree allows me to do is to ask a series of questions and depending upon those answers, move down the tree, until eventually I have some particular ouput answer, yes I go in the resteraunt, or no I do not Ok this is still seeming a little abstract to me, can we, can we maybe work through a very concrete example. Yeah, I think that makes a lot of sense. Let’s do a quiz. Ha, okay, let’s do a quiz.

1.1.9 Representation Quiz Question
Okay, so we’ve now seen an abstract example of decision trees. So let’s make certain that we understand it with a concrete example. So, to your right is a specific decision tree that looks at some of the features that we talked about. Whether you are occupied or not, whether the restaurant is occupied or not what type of restaurant it is. Your choices are pizza, Thai, or other. And whether the people inside look happy or not. The possible outputs are again binary either you don’t go or you do go, into the restaurant. On your left is a table which has six of the features that we’ve talked about. Whether the restaurant is occupied or not, the type of restaurant, whether it’s raining outside or not, whether your hungry or not. Whether you’re on a hot and whether the people inside look happy, and some values for each of those. And what we would like for you to do is to tell us what the output of this decision tree would be in each case. Alright, so the decision tree here is a, it’s a classifier, but we had another name. Oh, it’s a concept. Yes. Alright, and each row of this is a different time that we’re stopping at a restaurant, and the, the little values there summarize what is true about this particular situation. And, and you’re saying we need to then trace through this decision tree and figure out what class is, okay yeah, that’s what you said. Okay, I’m ready. Alright, perfect. Okay, that’s the quiz, go.

1.1.10 Representation Quiz Solution
You’ve got your answers. Let me tell you what we think the answers are. Now the nice thing about a decision tree sort of conceptually and intuitively, is that it really is asking a series of questions. So, we can simply look at these rows over here and the values that our features have and we can just follow down the path of the tree. So, in the first case. We have true. We have true for occupied, which means we want to go down the right side of the tree. And check on the type. So in the first case, the type is pizza. And so we go down the first branch and that means. We do not go down the tree. So, the output is no go. So, okay, so now, I got a different answer. So, I looked at this and I said happiness is true. And, the bottom box says happiness true, you go. Right. So, you got the wrong, you got what I’m going to tell you is the wrong answer by going from the bottom of the tree up. The way decision trees work is you always start at the root of the tree. That is the very top of the tree. And you ask the questions in tho, in that order. It just seems like it would be faster to start at the bottom. Yeah but then you would never look at anything else in the tree. Good point. All right so. If you start at the bottom, you can’t go up. Alright. So, okay, so let me see if I get this straight. So I’ll, I’ll do the, the second instance. The second instance, you say that we need to start at the top of the tree where it says occupied. And so now I look at the instance and the instance says that it’s false for occupied, so we go down that left branch and we hit no go. Oh wait but now I haven’t look at any of the other nodes. You don’t have to look at any of the other nodes because it turns out that if it’s not occupied you just don’t go into a restaurant. So you’re the type of person who doesn’t like to be the only person in a restaurant. Got it, all right, so that’s a no go. So that’s a no-go. That’s an important point, Michael. Actually, you might also notice that this whole tree, even if you look at every single feature in the tree, only has three of the attributes. It only looks at occupied. Type. And happiness. I see. So hot date is sort of irrelevant which is good, because in this case it’s not really changing from instance to instance anyway. True. And neither is hungry you might notice. Oh, I am kind of hungry. Although, I’m always hungry. Although raining does in fact change a little bit here and there. But it apparently it doesn’t matter. I see. Because you always take an umbrella. Got it. Okay, so let’s quickly do the other three and see if we we come to the same conclusion. Alright. Well all the instances that have occupied false we know those are no go, right away. Oh, good point. So we can do it kind of out of order. And the other ones are both occupied. One is tie and one is other. For the tie one we go. The other one, oh I see, for the other one we have to look at whether there’s happiness or not, and in this instance happiness is true. So we get on the right branch and we go. And we go. Exactly it. So we notice hot date doesn’t matter, hungry doesn’t matter and rainy doesn’t matter. And the only thing that matters are whether you’re occupied, what type of restaurant you’re at and whether you’re happy or not. Or whether the, the patrons in the restaurants are restaurant is, are happy or not. But, here’s the other thing about this. It’s not just about the features. Let’s tie it back in to the other things, that we mentioned in the beginning. This, in our case, this table actually represents our testing set. It’s the thing that we’re going to look at to determine whether we were right or wrong. These are the examples that we’re going to do to see whether we generalize or not. And this particular tree here is our candidate concept. So there’s lots and lots and lots of other trees that we might have used. We might have used a tree that also took, asked questions about whether it was rainy or not or asked questions about whether you were on a hot date or not. But we didn’t. We picked a specific tree that had only these three features and only asked in this particular way. So what we’re going to talk about next. Is how we might decide whether to choose this tree over any of the other possible number of trees that we might have chosen instead.

1.1.11 Example 2 - 20 Questions
Alright, so we understand at this point how to use these decision trees, but this is a class about machine learning, right? So we need to figure out how to make those decision trees happen as a result of processing a set of training data. So, it’s not clear to me how we’re going to make that leap. Let’s see if we can figure it out together. So, I’m going to play a game with you, Michael, and if we do the game well. Then I think we’ll have an idea of what a good algorithm might be for actually building a decision tree. So we’re going to play 20 questions. You’re familiar with 20 questions? Right, that’s the game where I’m allowed to ask yes/no questions to try to guess something that you’re thinking of, and if I take more than 20 of them, then I lose. Right, okay. So, here’s what I’m going to do. I’m going to think of a thing, and, you ask me questions and I’m going to answer them and we’ll see if you can guess what thing I’m thinking about. Okay, I’ve got something in my head. The first question, the typical first question is it animal, vegetable or mineral but that doesn’t seem like a yes/no question, so is it, is it an animal, or like a, a living creature. The answer is yes. Alright, is it a person? Is it a person? The answer is yes. Is it a famous person? Is it a famous person? That’s a deep philosophical question, but I’m going to say yes. Is it a famous person that we both know in like directly. Oh, who we both personally know directly, I do not believe so. Yeah. So, the answer is no. Is it a living person? Living person, no. So it is a dead, famous person. Is the person famous for, say being in the music industry. The answer yes. Did this person live during the 20th century? The answer is yes. Is the genre of music that the person was associated with, say hip hop or rap? No. Is the person a singer? Singer? Yes. Male or female, is the person female? Person female? No. That’s ten questions, Michael. Yes, the, the clock is ticking down, but I feel like I have narrowed it down quite a bit at this point. Did the person die since you’ve become a professor? So, say in the last How long have you been a professor? Too forever it sounds like feels like. Let’s see, recent death. And I’m going to say the answer is yes. Is the person’s name Michael? The answer is yes. Alright, alright, I think I’m on to it. Is it Michael Jackson? No. Woo hoo! Of course, it’s Michael Jackson. Alright, Thriller. Yes, Michael Jackson is the answer. Alright. So that was, that was very fun. And I’m very glad that I was able to solve it. [LAUGH] But it’s not clear to me how, this is going to give us an algorithm for building decision trees. Okay, so let’s think about that for a second. So you asked a bunch of questions, and you asked them in a particular order. Right? So, here’s a question I have for you. Why was the first question you asked me whether it was an animal or not? well, it seemed like I needed some way of narrowing down the space, and so I wanted to get as much information out of that question as I could to try to make progress towards figuring who it actually was. Right. So, animal is the first question and it was because it narrowed things down. So, your goal in asking questions was to narrow the possibilities, right? Sure, right because I only have 20 questions and then I’m, you know I’m out of it, so if I asked questions like You know, if I had said–started with, Is it someone named Michael, that would have been really bizarre. Right, and if the answer was no, it’s not clear that it would tell you anything at all. So, actually, that’s an interesting point. You started with animal; you could have started with Michael. And if I had said yes, that would have told you something. But if I had said No, it wouldn’t have told you hardly anything at all. Right? So animal is a better attribute, or feature, to ask about than Michael as a first question. Do you agree with that? Yeah, because it could have been like a stapler or something like that and then I, the Michael question would’ve been pretty silly. Exactly. So what about persons? So first you asked about animal. Then you asked about person. Why person? Right, because, because again it seemed like of the space of possibilities that I could think of person would help kind of again narrow things down. That I’d if it was the answer was yes, I would be able to focus there. If the answer was no I could focus some place else. Exactly. And so I think in fact we could, we have a general statement here. That each one of these questions. Person, famous, do we know this person, personally, so on and so forth. All make sense because they further narrow down the possibilities. And that bit about further is important. Because it implies that the usefulness of a question depends upon the answers that you got in the previous questions. So even though Michael is not a particularly good first question to ask, it’s a perfectly reasonable twelfth question to ask or however far down it is, given that you already know this person’s, this is an animal, a person, a famous person we don’t know personally who’s not living, who’s into music, etc., etc., etc. Okay, that make sense? Yeah. Okay. So I think then, we have the hints of an algorithm. So let’s try to write down that algorithm and, and see if it matches with our intuition.

1.1.12 Decision Trees Learning
Okay, so, inspired by 20 questions let’s try to write down exactly what it is that you did in going through your 20 questions to get your answer to discover Michael Jackson was the person I was thinking about. So, what is the very first thing you did? I tried to imagine a bunch of possible answers that you could be, could have in mind. And, I tried to think of a question that would help separate them into two roughly equal chunks. Okay, so what’s the way of putting that in the language that we’ve been using for supervised learning and classification so far? Oh I see. So if I had known in advance, here’s here’s 200 things that you might ask me about. And what their, what their attribute values are. What would be a question that would split that set in half, right? So, instead of just imagining a bunch of things, if I actually had a list, which I do in the case of a training set. Alright, so the first thing you did is you picked the best attribute that you could think of. And, by the way, you said something very particular here. You actually defined what best is. You said, that best is the same thing as splitting things roughly in half. So let’s revisit that in a moment. Okay, so the first thing you did is you picked the best atribute. You asked a question about it and then, depending upon the answer, you went and picked another attribute right? Does that seem fair Yeah. Okay. So, we think about decision trees, a way of talking about that is that you follow the path of the answer, and then lather, rinse and repeat. [LAUGH]. You went back and pick the best attribute, asked the question, followed the answer path, so on, and so on, and you kept doing that until what? Until, I narrowed the space of possibilities to, in this case, just one item. Right, so until you got to the answer. All right. And that is an algorithm. So you pick the best attribute, and you actually define what best attribute was. You want to pick one that would somehow eliminate at least half of the things which you might worry about and keep the other half in play. You asked a specific question. You followed the path to that and then you went back and you picked another attribute and so on and so forth until you got an answer that you wanted to. That’s an algorithm and that’s an algorithm that we might use to actually build a decision tree. And the only difference between what you did and what you would do with learning a decision a tree is that, since you don’t know in advance what the answer actually is going to be, because you don’t know what specific object I might be thinking of, you have to actually follow all possible paths at each time and think of all possible best next attributes until you completely can answer any question. Does that make sense? I see. So, right, so instead of just playing the game interactively, I kind of imagine all possible ways it could go and build that whole flow chart, that whole tree. Right, so, let’s see if we can do that with some pictures and I actually want to decide rather I really believe your definition of best. Okay? Sure. Alright, so, let’s do that.

1.1.13 Best Attribute Quiz Question
Alright, so let’s take a moment to have a quiz where we really delve deeply into what it means to have a best attribute. So something Michael and I have been throwing around that term, let’s see if we can define it a little bit more crisply. So, what you have in front of you are three different attributes that we might apply in our decision tree for sorting out our instances. So, at the top of the screen what you have is you have a cloud of instances. They are labelled either with red x or a green o, and that represents the label so that means that they are part of our training set, so this is what we’re using to build and to learn our Decision Tree. So, in the first case you have the set of instances being sorted into two piles. There are some xs and some os on the left and some xs and some os on the right. And the second case you have that same set of data being sorted so that all of it goes to the left and none of it goes to the right. And in the third case you have that same set of data that’s sorted so that a bunch of the xs end up on the left and a bunch of the os end up on the right. What I want you to do is to rank each one, where one is the best and three is the least best attribute to choose. Go.

1.1.14 Best Attribute Quiz Solution
Okay Michael. So, are you ready to give me a ranking Yes, yes, I think I am. Okay, well, if you give me a ranking then. So, did you say one was the best. One is the best and three is the least best. Alright, so I am really excited about the third cloud structure. The third attribute to be split on. Because, what it does. Is it takes all our x’s and o’s That need to have different labels and it puts them into two different buckets. One where they all get to be red x’s and the other where they all get to be green o’s. So I would say the far right one is ranked number one. I would agree with you and in fact I would say that infact we’re done. Yeah, it’s perfect. It is perfect, agreed. One is perfect. Or 3 is perfect in this case, because I gave it a one. Alright, so then, I think the worst one is also easy to pick, because if you look at the middle attribute, the attribute that’s shown in the middle, we take all the data, and we put it all on the left. So we really have just kicked the can down the road a little bit. There’s nothing. That this attribute splitting has done. So, I would call that the worst possible thing you could do. Which is to basically to do nothing. No, I think that’s right. and, in fact, it really is doing nothing. Right. Okay. So what about the first attribute? So, I’m going to put that at, you know, if the first one is too hot and the middle one is too cold, I would say this one is, wait, no, no. [LAUGH] I was going down the Goldilocks path. So, so this one is sort of in between. In that it splits things so you have smaller sets of data to deal with, but it hasn’t really improved our ability to tell the reds and the greens apart. So in fact, I’d almost want to put this as three also but I’ll put it as two. Okay. I think an argument could be making it three. An argument could be made for making it two. Your point is actually pretty good, right? We have eight red things and eight green things up here. And the kind of distribution between them, sort of half red and, half red x’s, half green o’s, we have the same distribution after we go through this attribute here. So it does some splitting, but it’s still, well you still end up with half red, half green, half x, half o. So, that’s not a lot of help, but it’s certainly better than doing absolutely nothing. Well is it though? I mean, it seems it could also be the case. That what we’ve done is that we’re now splitting on that has provided no valid information, and therefore can only contribute to overfitting. That’s that’s a good point. That’s a good point. So, do you want to change your answer to three? I don’t know, what did you want the answer to be? It seems to me that the first one and the second one are just plain bad. They are just plain bad. The question is whether one is, more bad than the other. I, I don’t know. I don’t know how to judge. Well I’ll tell you, I would accept either two or a three as an answer here. I think you can make an argument either way. And I think you actually made both arguments. That’s very nice of you. So. Thank you. You’re very welcome. So this is perfect. This is the House of Representatives and this is the Senate. [LAUGH] What do you mean they’re? Oh. So, there you go. Okay. [LAUGH] Not exactly sure what you mean, but it seems somehow denigrating to our political system. It is not at all denigrating. It is, it is, I would call it incisive political satire.

1.1.15 Decision Trees Expressiveness AND
Okay. So Michael, for the last 15 minutes or so we’ve been talking about decision trees, sort of in the abstract without saying too much about the kinds of functions they can actually represent. So, for the next few minutes or so I want to talk a little bit about not just decision trees in the abstract, but exactly how expressive they can be. Is that okay? Yeah, I think that would be really helpful. I think so too. So in fact, I want to look at a set of functions, and in particular I’m interested in looking at Boolean functions. Boolean. So what’s your favorite simple Boolean function? Implication. What’s your other favorite simple boolean function? Well I like Nor. Right. So what I just heard you say is you like And, so let’s do that one. Oh, that’s great. That is my favorite. All right. So, in fact, let’s do very simple And. So, how does And work, right? So, you’ve got two attributes. Let’s say A and B. And, this expression, A and B, is true when? When A and B are true. When they are both true at the same time. Right, exactly. So, how would you build a decision tree, given that there are only two attribute, A and B, to represent that function? Okay so I’d have a note that’s A and B. And if that’s true. Nope, you’re not allowed to do that. Oh. Every node can be, have at most one attribute. All right well let’s let’s put A in an attribute. Okay, so here’s a little node. Let’s call it A. Okay. Now what? And well I mean, for it to be a node it needs to have the little two branchy things. True and false. Okay. All right, so how about true on the left and false on the right? Sure, as long as you label them. So, all right. So then A, if A is true, okay, I don’t know. But oh, but if A is false, then we know that A and B must be false. Doesn’t matter what B is. So we can just put a leaf under the F. That’s correct. All right, I like that. Okay. What about when A is true? What, is that an F? That is an F, and that is a minus sign for false. Oh, a minus sign. I get it, okay. I thought you were just not done drawing the F yet. good. All right. So, oh yeah. On the true side then, I don’t think we know. So I think we need to split on B also. Okay. So put a little B under there. All right, done. All right, and then true-false split on B. All right and so now we’ve got these two cases. So if B is false, then again, it doesn’t matter what A was but A turns out to be true. But it’s still the, the, it should be a minus sign underneath that. Okay. So it’s not A and B is not true. But if A is true and B is true then A and B is true. So there should be a plus sign on the left. That’s exactly right. Woo. So clearly decision trees proof by, we just drew it here, can represent the Boolean function And. Sure. [CROSSTALK]. You said something int, you said something interesting, Michael. You said it doesn’t matter what A is if B is false. So what would happen if I switch, A and B around. That’s the same. So if B, okay, in the beginning, we say, if B is false, it’s false. If B is true, we check A. If A is false, then it’s false. But if A is true, then it’s true. So it actually still represents exactly the same function, A and B. Oh, because A and B is collaborative. No, commutative. Yes? No? Hello? It’s one of those things. It’s commutative. As opposed to associative. As opposed to what? Associative. Well I mean it, it’s that too. But I mean, it’s the reason that you can just switch those two things and it didn’t make a difference is because they play the same role in the function. That’s true in, in terms of representation of the decision trees. You know, it doesn’t really matter which attribute you pick or the order in which you do them. You might get a better tree or a worse tree or a longer tree or a shorter tree. But for something simple like, two valued And, it really just doesn’t matter. Okay. kind of neat, huh? Yeah.

1.1.16 Decision Trees Expressiveness OR
Okay, so we can do A and B. So, let’s pick another function. What’s your other favorite, Boolean function of two variables? Well, if we’re doing two attributes, you know, or is the other really nice one. I like or. So, let’s see. Do you think, now that we’ve shown that we can do A and B, could we do A or B? I feel like we could do A or B. because there’s that nice De Morgen’s Law relationship between them but but let’s just, how about this, can you erase the tags at the bottom? Like that? Oh, yeah but that’s not going to work, is it? So, so if B and A are both true, so the left branch, then the, the, the leaf should be plus? Yes. And if B is true and A is false then the tag should be plus. Yes. And if B is false, I want to say false but we don’t know because A could be true and A or B is true if either of the two of them are true. Oh okay, so maybe the mistake here is just trying to use exactly the same thing. So let’s just erase it. All right. And start from scratch. So what would you do starting from scratch? I’d split on B again. Okay. True False or False True. True False. True False. All right, and now if B is true. We know that A or B is true, so we can put a plus under the left side. Right. But if B is false. Mm-hm. Then we need to split on A. Okay. And if A is true, then we’re golden, we get the Or is true. And if A is false, then it is false. Very good, and that represents A or B. That represents the function, the logical function A or B, yeah. Right. What happens if I swap A and B around, does it still work? I mean, my, my, since they’re collaborative or commutative. Since they’re commutative I want to say it shouldn’t make a difference, but let’s just double check that. So if A is true then the output’s true. If A is false and B is true then the output’s true. And if A and B are both false, then the answer’s false, yeah, totally. Excellent. And, you know, if I hadn’t erased it you would see that the two trees actually look very, very much alike. They’re sort of mirror each, of each other. If you just swapped around. Yeah the, yeah exactly, they’re mirrors of one another.

1.1.17 Decision Trees Expressiveness XOR
[LAUGH] Okay, let’s pick one more Boolean function to do. What function am I thinking of? XOR is always good. Let’s do XOR. So what does XOR mean? Remind me. Okay. XOR is exclusive or, which means it’s true if A is true or B is true, but not if they’re both true, so it has elements of both or and and in it. Right. And I think of XOR usually when I’m, when I’m, like, playing with light switches in my house. If I have a, a light that’s activated by two different light switches, it’s usually an XOR function. If one is up, the light’s on, if the other one’s up, the lights on, but if they’re both up, the light goes back off. Right. The other thing when I think of XOR is when people say or most of the time when people say or, when they’re talking, they mean xor. Really? Yeah. So a lot of times when people say or in English, they really mean xor. They say, well do you want to go to the movies or do you want to go swimming? Do want to eat chicken or do you want to eat fish? And really, they’re saying either or. I see, you want one or the other, but you can’t have chicken and fish or go swimming and the movies. No, those things are not possible to do together. Got it. Okay. All right, so how would you do XOR? We got, we still have our- Well, I would start with the- Or, because it’s a lot like or. So, what would you want to do? Okay, so to do XOR, we can split on A. Okay. And there’s a true branch and a false branch. And what happened with and and or at this point is, there is at least one branch where we actually knew the answer, at this point, but I don’t think that’s true here. That’s right. So, so if A is true, the output might be true or false. It depends on B. And if A is false, the output might be False or True. It depends on B. So I- This is exactly right. So I guess in both cases we still have to split on B. Okay. All right. So, now it should be relatively easy in the sense that there’s a separate leaf for all possible inputs. And so we can just write the XOR function on the bottom. So if A and B are both true, then the output is false because it’s exclusive. If A is true and B is false, then it should be true, because A is true. If A is false and B is true, then it should be true because B is true. And if A and B are both false then it should be false. That’s right. And by the way, if you were to think about it for a little while, it would probably occur to you that’s this tree is just a another representation of the full truth table. Very nice. And in fact, if we wanted to do or again, we could say, well I was very smart last time, but I could actually write or like this. And then just fill out the values at the bottom. If A is true and B is true, then yes. IfAistrueandBisfalse,thenyes. IfAisfalseandBistrue,thenyes. IfAisfalseandBisfalse,thenno. And this is a perfectly legitimate a decision tree to represent or because it basically is just another simple representation of the truth table, but as we pointed out when did last time, it’s more of a decision tree than we actually need. I can see that. Because in particular, we don’t actually need this. All right. So this little difference here between writing out the entire truth table on the left, as we did for XOR, and not having to write out the entire decision tree on the right for o,r actually isn’t going to turn out to matter when we try to do things either more complicated or with more attributes that we did for the simple and, or, and XOR. Would you like to see? Yeah, that sounds really cool. Beautiful.

1.1.18 Decision Tree Expressiveness
So, we saw before when we looked at AND and OR versus XOR that in the case of AND and OR we only needed two nodes but in the case of XOR we needed three. The difference between two and three is not that big, but it actually does turn out to be big if you start thinking about having more than simply two attributes. So, let’s look at generalized versions of OR and generalized versions of XOR and see if we can see how the size of the decision tree grows differently. So in the case of an n version of OR. That is we have n attributes as opposed to just two. We might call that the any function. That is a function were any of the variables are true then the output is true. We can see that the decision tree for that has a very particular and kind of interesting form. Any ideas Michael about what that decision tree looks like? So, well. So going off of the way you described OR in the two case. We can start with that. And you. You pick Pick one of the variables. And if its true then yeah. Any of them is true since the leaf is true. What happens if its false? Well, then we have to check what everything that’s left. [LAUGH] So then we move on to one of the other attributes like A2. mm hm. And again, if it’s true, it’s true and if it’s false then we don’t know. Look at A3. Good idea. This could take some time. Yes, oh that was actually an interesting point. Let’s say if there were only three, we would be done. Right? Right. But wait, what if there were five? Then we need one more node. What if there were n? Then we need n minus 4 more nodes. Right, so what you end up with in this case is a nice little structure around the decision tree. And how many nodes do we need? Looks like one for each attribute, so that would be n. n nodes, exactly right. So we have a term for this sort of thing, the size of the decision tree is, in fact, linear. In n. And that’s for any. Now what about an n version of XOR? Mm. So XOR is, if one is true but the other one’s not true then it’s true. And if they’re both true. Yeah I don’t. It’s not clear headed. Generalize that. So why not hmm. So, while the usual general version of this we like to think of as parity. All parity is a way of counting, so there’s usual two forms of parity that we worry about. Either even parity or odd parity. So let’s pick one, it doesn’t matter. Let’s say. Odd. I like that, we’ll do odd parity. And all that works out to be in this case is, if the number of attributes that are true is an odd number, then the output of the function is true, otherwise it’s false. Got it? Got it. Okay, so, how would we make that decision tree work? Ooh. Well, we got to split on something and there all the same, so let’s split on A1 again. Okay. So what do we do if A1 is true, versus being false. We don’t know much if A1 is true. We have to look at everybody else. Right. So let’s look at A2. What if A2 is true versus false? Well if A1 and A2 are true then, then the output is going to be whatever the parity of all the remaining variables are. So you still have to do that. Uh-huh, yup. And I’m already running out of room, so let’s pretend there’s only three variables. What’s the output? [LAUGH] All right, so the far left. Is there’s three trues which is odd so the output is true. Yep. The next leaf over, only two trues. A1 is true, A2 is true, but A3 is false, so that’s two trues which is is even so the answer’s false. Um-huh. Is this going to, is this pattern continuing? Now we’ve got. No, so then it’s false again because we’ve got two trues and a false to get to the next leaf. Mm-hm. And we’ve got one true to get to the next leaf so that’s true. Oh, that looks like XOR. It looks just like XOR. In fact, each one of these sub trees is kind of a version of XOR isn’t it? Now what we have is, we have to do the same thing on the right. So we gotta redo A2, and we’re going to be in the same situation before. And we’re going to start drawing on top of each other because. [LAUGH] there’s just not enough room. Hm, so, what’s the answer to the one on the very right. Where all of them is false. All of them are false. So that’s, an even number of trues. Zero is even. So that’s false. Okay, so in the case where only A3 is true, it’s true and we just keep going on and on and on again. Now imagine what would happen, in fact let me ask you Michael, what would happen if we had four attributes instead of three. Then we would be really tired of this game. Yes, and I am already tired of this game so the question is, can you. We get a whole another, a whole other level of this tree. Yep. We have the, it just goes on and on and on and nobody wants to think about it anymore. [LAUGH] So, how many nodes do you think there are? Well, for three there was one, two, three, four, five, six, seven. Which seems suspiciously like one less than the power of two. Mm-hm. And that is exactly right. You need more or less 2 to the n nodes. Or. 2 to the n, maybe, minus 1. Yeah. So let’s just say big O of 2 to the n. Everyone watching this is a computer scientist to they know what they’re doing. Okay so, you need an exponential therefore, as opposed to linear number of nodes. Gad. Yeah, so you very, very quickly run out of room here. You very, very quickly have a really, really big tree because it’s growing exponentially. So, XOR is an exponential problem and is also known as hard. Whereas OR, at least in terms of space that you need, it’s a relatively easy one. This is linear. We have another name for exponential and that is evil. Evil, evil, evil. And it’s evil because it’s a very difficult problem. There is no clever way to pick the right attributes in order to give you an answer. You have to look at every single thing. That’s what make this kind of problem difficult. So, just as a general point, Michael, I want to make, is that we hope that in our machine learning problems we’re looking at things that are more like any than we are looking at things that are more like parity. Because otherwise, we’re going to need to ask a lot of questions in order to answer the, the parity questions. And we can’t be particularly clever about how we do it. Though, if we were kind of clever and added another attribute, which is like, the sum of all the other attribute values, that would make it not so bad again. So maybe it’s just a, it’s just a kind of, bad way of writing the problem down. Well, you know, what they say about AI is that the hardest problem is coming up with a good representation. So what you just did is, you came up with a better representation, where you created some new pair, new variable. Let’s call it B, which is just the sum of all of the As, where we pretend that I don’t know, true is one and false is zero. This is actually a really good idea. It’s also called cheating. [LAUGH] Because you [LAUGH] got to solve the problem by picking the best representation in the first place. But you know what? It’s a good point, that in order for a machine running to work, you either need an easy problem or you need to find a clever way of cheating. So, let’s come back and think about that throughout all the rest of the lessons. What’s the best way to cheat?

1.1.19 Decision Tree Expressiveness Quiz Question
All right, so what that last little exercise showed is that XOR, in XOR parody, is hard. It’s exponential. But that kind of provides us a little bit of a hint, right? We know that XOR is hard and we know that OR is easy. At least in terms of the number of nodes you need, right? But, we don’t know, going in, what a particular function is. So we never know whether the decision tree that we’re going to have to build is going to be an easy one. That is something linear, say. Or a hard one, something that’s exponential. So this brings me to a key question that I want to ask, which is, exactly how expressive is a decision tree. And this is what I really mean by this. Not just what kind of functions it kind of represent. But, if we’re going to be searching over all possible decision trees in order to find the right one, how many decision trees do we have to worry about to look at? So, let’s go back and look at, take the XOR case again and just speak more generally. Let’s imagine that we once again, we have n attributes. Here’s my question to you, Michael. How many decision trees are there? And look, I’m going to make it easy for you, Michael. They’re not just attributes, they’re Boolean attributes. Thanks. Okay? And they’re not just Boolean attributes, but the output is also Boolean. Got it? Sure. But how many trees? So it’s, I’m going to go with a lot. Okay. A lot. Define a lot. Define a lot. So, alright, well, there’s n choices for which node to split on first. Yeah. And then, for each of those, there’s n minus 1 to split on next. So I feel like that could be an n factorial kind of thing. Maybe. I like that. And then, even after we’ve done all that, then we have an exponential number of leaves. And for each of those leaves, we could fill in either true or false. So it’s going to be exponential in that too. Hm, so let me see if I got that right. So you said we have to pick each attribute at every level. And so you see something that you think is probably going to be, you know? Some kind of commutatorial question here. So, let’s say n factorial, and that’s going to just build the nodes. That’s just the nodes. Well, once you have the nodes, you still have to figure out the answers. And so, this is exponential because factorial is exponential. And this is also exponential. Huh. So let’s see if we can write that down. So let me propose a way to think about this, Michael. You’re exactly right the way you’re thinking. So, let’s see if we can be a little bit more concrete about it. So, we have Boolean inputs and we have Boolean outputs, so this is just like AND, it’s just like OR, it’s just like XOR, so, whenever we’re dealing with Boolean functions, we can write down a truth table. So let’s think about what the truth table looks like in this case. [SOUND] Alright, so, let’s look at the truth table. So what a truth table will give me is, for, the way a truth table normally works is you write out, each of the attributes. So, attribute one, attribute two, attribute three, and dot dot dot. And there’s n of those, okay? We did this a little earlier. When we did our decision tree. When we tried to figure out whether I was on a hot date or not. And then you have some kind of output or answer. So, each of these attributes could take on true or false. So one kind of input that we may get would be say all trues. Right? But we also might get all trues, except for one false at the end. Or maybe the first one’s false and all the rest of them are true, and so on, and so forth. And each one of those possibilites is another row in our table. And that can just go on for we don’t know how long. So we have any number of rows here and my question to you is how many rows? Go.

1.1.20 Decision Tree Expressiveness Quiz Solution
So, we’re back. What’s the answer Micheal? How many rows do we have? So if it was just one variable we’re splitting on, then it need to be true or false, so, that’s two rows. If it was two variables, then there’s four combinations and three, would be eight, combinations. So, generalizing the end, it ought to be 2 to the n. That’s exactly right, there are 2 to the n different rows. And, that’s what always happen when we’re dealing with n, you know, n attributes, n boolean attributes. There’s always 2 to the n possibility. Okay, so I get just halfway there and I get to your point about, combinatorial choices, among the attributes. But ,that’s only the number of rows that we have. There’s another question ,we need to ask which is, exactly how big is the truth table itself?

1.1.21 Decision Tree Expressiveness Quiz 2 Question
Alright, so here’s the question for you. We know we have 2DN, different possible instances we might see. That is two to the end, different ways we might assign different values to the attributes. But, that still doesn’t tell us how many decision trees we may have, or how many different functions we might have. So, if we have 2DN rows, here’s my question. How many different ways might we fill out. This column over here of outputs? Remember, an output can be either true or false. Go.

1.1.22 Decision Tree Expressiveness Quiz 2 Solution
Okay, Michael, what’s your answer? Alright. So. Again, a lot feels like a good answer, it’s already written down on the left. But it’s also wait, wait, may be we can quantify this. So if it were. Maybe one way to think about this is if each of the, each of those empty boxes there, is either true or false. It’s kind of like a bit. And we’re asking how many different bit patterns can we make? And in general, it’s two to the number of positions, but here the number of positions is 2 to the n. So it ought to be 2, to the 2 to the n. Which is that the same as 4 to the n? No. Okay. But you’re right. It’s 2 to the 2 to the n. So it’s a double exponential and it’s not the same thing as 4 to the nth. It’s actually 2 to the 2 to the nth. Now how big of a number do you think that is Michael? I’m going to go again to my go to place and just say a lot. It is, in fact, a lot and I’m going, I actually, I’m going to look over here, and I’m going to tell you. That for even a small value of n, this gets to be a really big number. So for, for one, it’s 2 to the 2 to the 1, which is 4. That’s not a big number. That’strue. Whatabouttwo? Fortwo,it’s2tothe2tothe2. So2tothe2is4,soit’s2tothe4, which is 16. What about three? Alright, so that’s two to the 8th, which is 256? Mm-hm. So that’s growing pretty fast, don’t you think? Sure, but those aren’t big numbers yet. What if I told you, [LAUGH] that for n equals 6, 2 to the 2 to the n was, I’m going to start writing it, okay? 18466744073709551616. Holy monkeys. Yes, that is in fact the technical term for this number, it’s a holy monkey. It is a very, very big number. So 2 to the n grows very fast. We already called that evil. 2 to the 2 to the n is a double exponential and it’s super evil. It grows very, very, very, very, very fast. So what’s the point of this exercise, Michael? It’s, it’s to point that the space of decision trees, the hypothesis space that we’ve chosen, is very expressive because there’s lots of different functions that you can represent. But that also means we have to have some clever way to search among them. And that gets us back to our notion of an algorithm with actually going to very smartly go through and pick out which decision tree. Because if we aren’t very smart about it and we start eliminating whole decision trees along the way. Then we’re going to have to look it to billions upon, billions upon, billions upon, billion upon, billion of possible decision choice.

1.1.23 ID3
So now, we have an intuition of best, and how we want to split. We’ve, we’ve looked over, Michael’s proposed, the high-level algorithm for how we would build a decision tree. And I think we have enough information now that we can actually do, a real specific algorithm. So, let’s write that down. And the particular algorithm that Michael proposed is a kind of generic version of something that’s called ID3. So let me write down what that algorithm is, and we can talk about it. Okay, so here’s the ID3 algorithm. You’re simply going to keep looping forever until you’ve solved the problem. At each step, you’re going to pick the best attribute, and we’re going to define what we mean by best. There are a couple of different ways we might, we might define best in a moment. And then, given the best attribute that splits the data the way that we want, it does all those things that we talked about, assign that as a decision attribute for node. And then for each value that the attribute A can take on, create a descendent of node. Sort the training examples to those leaves based upon exactly what values they take on, and if you’ve perfectly classified your training set, then you stop. Otherwise, you iterate over each of those leaves, picking the best attribute in turn for the training examples that were sorted into that leaf, and you keep doing that. Building up the tree until you’re done. So that’s the ID3 algorithm. And the key bit that we have to expand upon in this case, is exactly what it means to have a best attribute. All right so, what is exactly, what exactly is it that we mean by best attribute? So, there are lots of possibilities, that you can come up with. The one that is most common, and the one I want you to think about the most, is what’s called information gain. So information gain is simply a mathematical way to capture the amount of information that i want to gain by picking particular attribute, funnily enough. But what it really talks about is the reduction in the randomness, over the labels that you have with set of data, based upon the knowing the value of particular attribute. So the formula’s simply this. The information gain over S and A where S is the collection of training examples that you’re looking at. And A, as a particular attribute, is simply defined as the entropy, with respect to the labels, of the set of training examples, you have S, minus, sort of, the expected or average entropy that you would have over each set of examples that you have with a particular value. Does that make sense to you, Michael? So what we’re doing, we’re picking an attribute and that attribute could have a bunch of different values, like true or false, or short, medium, tall? Right. And. And that’s represented by v. Okay, each of those is a different v. And then we’re saying okay, for over those leaves, we’re going to do this entropy thing again. Mm-hm. And we right. So what, and what is entropy? entropy. So, we’ll talk about entropy later on in the class in some detail and define it exactly and mathematically. And some of you probably already know what, what entropy is, but for those of you who don’t, it’s exactly a measure of randomness. So if I have a coin, let’s say a two-headed coin. It can be heads or tails, and I don’t know anything about the coin except that it’s probably fair. If I were to flip the coin, what’s the probability that it would end up heads versus tails? A half. It’s a half, exactly, if it’s a fair coin it’s a half. Which means that I have no basis, going into flipping the coin, to guess either way whether it’s heads or it’s tails. And so that has a lot of entropy. In fact it has exactly what’s called one bit of entropy. On the other hand, let’s imagine that I have a coin that has heads on both sides. Then, before I even flip the coin, I already know what the outcome’s going to be. It’s going to come up heads. So what’s the probability of it coming up with heads? It’s. One. One. So that actually has no information, no randomness, no entropy whatsoever. And has zero bits of entropy. So, when I look at this set of examples that I have, and the set of labels I have, I can count the number that are coming up, let’s say, red x’s. Versus the ones that are coming up green o’s. And if those are evenly split, then the entropy of them is maximal, because if I were to close my eyes and reach for an instance, I have no way of knowing beforehand whether I’m more likely to get an x or I’m more likely to get an o. On the other hand, if I have all the x’s in together, then I already know before I even reach in that I’m going to end up with an x. So as I have more of one label than the other the amount of entropy goes down. That is, I have more information going in. Does that make sense, Michael? I think so. So, is there, can, maybe we can say what the formula is for this, or, or? Sure. What is the formula for it? You should remember. It’s, if we have, well, I’m not sure what the notation ought to be with these S’s but it has something to do with Log P log, no wait, it’s P(log)P. Mm-hm. So the actual formula for entropy, using the same notation that we’re using for information gain is simply the sum, over all the possible values that you might see, of the probability of you seeing that value, times the log of the probability of you seeing that value, times minus one. And I don’t want to get into the details here. We’re going to go into a lot more details about this later when we get further on in the class with randomize optimization, where entropy’s going to matter a lot. But for now, I just, you have, I want you to have the intuition that this is a measure of information. This is the measure of randomness in some variable that you haven’t seen. It’s the likelihood of you already knowing what you’re going to get if you close your eyes and pick one of the training examples, versus you not knowing what you’re going to get. If you close your eyes and you picked one of the training examples. Okay? Alright. So, well, so, okay, so then in the practice, trees that you had given us before, it was the case that we worked, we wanted to prefer splits that I guess, made things less random, right? So if things were all mixed together, the reds and the greens, after the split if it was all reds on one side and all greens on the other. Then each of those two sides would have very, what? They would have very low entropy, even though when we started out before the split we had high entropy. Right, that’s exactly right. So if you, if you remember the, the, three examples before. One of them, it was the case that all of the samples went down the left side of the tree. So the amount of entropy that we had, didn’t change at all. So there was no gain in using that attribute. In another case, we split the data in half. But in each case, we had half of the x’s and half of the o’s together, on both sides of the split. Which means that the total amount of entropy actually didn’t change at all. Even though we split the data. And in the final case, the best one, we still split the data in half, but since all of the x’s ended up on one side and all of the o’s ended up on the other side, we had to entropy or no randomness left whatsoever. And that gave us the maximum amount of information gain. So is that how we’re choosing the best attribute? The one with the maximum gain? Exactly. So the goal is to maximize over the entropy gain. And that’s the best attribute.

1.1.24 ID3 Bias
So, we’ve got a whole bunch of trees we have to look at, Michael. And were going to have to come up with some clever way to look through them. And this get’s us back, something that we’ve talked about before, which is the notion of bias. And in particular, the notion of inductive bias. Now, just as a quick refresher, I’m want to remind you that there is two kind of biases we worrying about when we think about algorithms that are searching through space. One is what’s called a restriction bias. The other is called preference bias. So a restriction bias is nothing more than the hypothesis set that you actually care about. So in this case, with the decision trees, the hypothesis set is all possible decision trees. Okay? That means we’re not considering, y equals 2x plus 3. We’re not considering quadratic equations. We’re not considering non- boolean functions of a certain type. We’re only considering decision trees, and all that they can represent. And nothing else. Okay? So that’s already a restriction bias and it’s important. Because, instead of looking at the infinite number uncountably infinite number of functions that are out there, that we might consider. We’re only going to consider those that can be represented by a decision tree over in, you know, all the cases we’ve given so far discreet variable. But a preference bias is something that’s just as important. And it tells us what source of hypotheses from this hypothesis set we prefer, and that is really at the heart of inductive bias. So Michael, given that, what would you say is the inductive bias of the ID3 algorithm? That is, given a whole bunch of decision trees, which decision trees would ID3 prefer, over others? So, it definitely tries, since it’s, since it’s making it’s decisions top down. It’s going to be more likely to produce a tree that has basically good splits near the top than a tree that has bad splits at the top. Even if the two trees can represent the same function. Good point. So good splits near the top. Alright. And you said something very important there Michael. Given two decision trees that are both correct. They both represent the, the function that we might care about. It would prefer the one that had the better split near the top. Okay, so any other preferences? Any other inductive bias on the, on the ID3 algorithm. It prefers ones that model the data better to ones that model the data worse. Right. So this is one that people often forget: it prefers correct ones to incorrect ones. So, given a tree that has very good splits at the top but produces the wrong answer. It will not take that one over one that doesn’t have as good splits at the top, but does give you the correct answer. So that’s really, those are really the two main things that are the inductive bias for ID3. Although, when you put those two together, in particular when you look at the first one, there’s sort of a third one that comes out as well, which is ID3 algorithm tends to prefer shorter trees to longer trees. Now, that preference for shorter trees actually comes naturally from the fact that you’re doing good splits at the top. Because you’re going to take trees that actually separate the data well by labels, you’re going to tend to come to the answer faster than you would if you didn’t do that. So, if you go back to the example where we went before, where one of the attributes doesn’t split the data at all, that is not something that ID3 would go for, and it would in fact create a longer and unnecessarily longer tree. So it tends to prefer shorter trees over longer trees. So long as they’re correct and they give you good splits near the top of the tree.

1.1.25 Decision Trees Continuous Attributes
Alright. So, we’ve actually done pretty well. So through all of this, we finally figured out what decision trees actually are. We know what they represent. We know how expressive they are. We have an algorithm that let’s us build the decision trees in an effective way. We’ve done just about everything there is to do with decision trees, but there is still a couple of open questions that I want to think about. So, here’s a couple of them and I want you to, to think about and then we’ll discuss them. So, so far all of our examples that we’ve used. All the the things we’ve been thinking about for good pedagogical reasons. We had not only discreet outputs but we also had discreet inputs. So one question we might ask ourselves, is what happens if we have, continuous attributes? So Michael, let me ask you this. Let’s say we had some continuous attributes. We weren’t just asking whether someone’s an animal or whether they’re human or whether it’s raining outside or we really cared about age or weight or distance or anything else that might have a continuous attribute. How are we going to make that work in a decision tree? Well, I guess the literal way to do it would be for something like age to have a branching factor that’s equal to the number of possible ages. Okay, so that’s one, one possibility. So we stick in age and then we have one. 1.0, we have one for 1.1, we have one for 1.11, we have one for 1.111 Ahh, I see. Alright. Well, at the very least, okay. Heheheh. What if, what if we only included ages that were in the training set? Presumably there’s at least a finite number of those. Oh, we could do that. We could just do that, except what are we going to do then when we come up with something in the future that wasn’t in the training session. Oh, right. Can we look at the testing set? No were not allowed to look at the testing set. That is cheating, and not the kind of good cheating that we do when we pic good representation. Okay, fair enough. Well we could, we could Ranges. What about ranges? Isn’t that the way we cover more than just individual values? Give me an example. Say ages you know, in the 20s. Okay, so, huh. How would we represent that with a decision tree? Let’s say in the 20s. Age. How we do that. You could do like age, element sign, bracket. 20. 20 comma 21, or, or 29 or 30 right per end. Yeah it’s too much. Why don’t I just say age Is between less is, let’s see, greater than or equal to, 20 and, less than 30. And just draw a big oval for that. Alright? So that’s a range, so that’s all numbers between, 20 and 30 inclusive of 20 but not 30 Right. Yeah. And what’s good about that is that’s a question. You are either in your 20s or you are not. So, the output of that is actually true or false. So, I guess the good news there is that now we know how to evaluate attributes like that because we have a formula from ID3 that tells you what to do. But seems like there’s an awful lot of different ones to check. Right, and in fact if it’s truly a continuous variable, there are in principal an infinite number of them checked. But we can do now the sort of cheating you wanted to do before. We can just look at the training set, and we could try to pick questions that cover the sorts of data in the training set. So, for example, if all of the values are in the 20s, then there is no point of even asking the question. You will start just split instead upon values that were, say less than 25 or greater than 25, and you could imagine all kinds of ways where you might do that. You might look at all of the values that show up in the training set, and say well, I am going to do a binary search. So, I am just going to create an attribute for Less than half of whatever is in the training set or greater than half of whatever the range is in the training set. Does that make sense? Yeah, that’s clever. Right. Thank you. I just made that up on the spot. Okay, so you do those sorts of things and that’s how you would deal with continuous attributes. That brings me to a next question, I’m going to actually do this as a quiz because I want an answer from our audience.

1.1.26 Decision Trees Other Considerations Quiz Question
So, here’s the next question I want to ask you, simple true or false question. Does it make sense to repeat an attribute along any given path in the tree? So, if you we pick some attribute like A, should we ever ask a question about A again? Now, I mean something very specific about, by that. I mean, down a particular path of the tree, not just anywhere else in the tree. So, in particular, I mean this. So, I ask a question about A, then I ask a question about B, and then I ask a question about A again. That’s the question I’m asking. Not whether A might appear more than once in the tree. So, for example, you might have been the case where A shows up more than once in the tree, but not along the same path. So, in the second case over here, A shows up more than once, but they really don’t really have anything to do with one another because once you’ve answered B, you will only ever ask the question about A once. So, my question to you is, does it make sense to repeat A more than once along a particular path in the tree? Yes or no? Answer the question.

1.1.27 Decision Trees Other Considerations Quiz Solution
Okay, Michael, what’s the answer? So, alright. Does it make sense to repeat, an attribute along a path in the tree? So, it seems like it could be no, [SOUND] in that, you know, if we’re looking at attributes like, you know, is a true, then later we would ask again is a true because we would already have known the answer to that. Right, and by the way, information gain will pick that for you automatically. It doesn’t have to be a special thing in the algorithm, if, if, you consider, an attribute that you’ve already split on, then you’re not going to gain any information, so it’s going to be the worst thing, to split on. Exactly. Alright, but it, Okay, doing good. But it seems like maybe you’re trying to lead us on because ,this we’re in the continuous attributes portion of our show. Okay, well what’s the answer there? Is the answer not also false? Well we wouldn’t want to ask the same question, about the same attribute. So, we wouldn’t have age, between 20 an 30, and then later ask again, age ,between 20 and 30. But ,maybe we want to ask, you know, given that we are less than 20, we’re, are we teenagers or not, so we might have a different range, on age later in the tree. So, that’s exactly right, Michael. So, the answer is no, it does not make sense ,to repeat an attribute along a path of the tree, for discrete, value trees. However, for continuous [UNKNOWN] attributes, it does make sense. Because, what you’re actually doing, is asking a different question. So, one way to think about this, is that the question is age in the 20’s or not. Is actually ,a discreet valued attribute that you’ve just created, for the purposes of the decision tree. So, asking that question doesn’t make sense but asking a different question, about age, does in fact make sense. So ,once you know, that you are not in the 20’s you might ask well am I less than, 20 years old? Maybe a teenager or am I greater than 40. How old am I, 44? Greater than 44, in which case, I’m old. So if it’s, if you ask, [LAUGH] the tree that you drew isn’t quite right though, right? Yeah, it is. because, if you go down the false branch, it means you are less than 20. No, I can be greater than 30. Oh, good one. Yes, I’m very clever, or at least I accidentally got it right. One of the two, it’s the same thing. Okay, so there you go.

1.1.28 Decision Trees Other Considerations
So, we’ve answered the thing about continuous attributes. Now, here’s another thing. When do we really stop? When we get all the answers right. When all the training examples are in the right category. Class. Right, so the the answer in the algorithm is when everything is classified correctly. That’s a pretty good answer, Michael. But what if we have noise in our data? What if it’s the case that we have two examples of the same object, the same instance, but they have two different labels? Then this will never be the case. Oh. So, then our algorithm goes into an infinite loop. Which seems like a bad idea. So we could have, we could, we could just say, or we’ve run out of attributes. [LAUGH] [LAUGH] Or we’ve run out of attributes. That’s one way of doing it. In fact, that, that was, that’s going to have to happen at some point, right? That’s probably a slightly better answer. Although that doesn’t help us in the case where we have continuous attributes and we might ask an infinite number of questions. So we probably need a slightly better criteria. Don’t you think? Hm. So, what got us down this path, was thinking about what happens if we have noise. Why would we be worried about having noise anyway? Noise anyway. Well, I guess the training data might have gotten corrupted a little bit or maybe somebody copied something down wrong. Right, so since that’s always a possibility, does it really make sense to trust the data completely, and go all the way to the point where we perfectly classify the training data? But Charles, if we can’t trust our data, what can we trust? Well, we can trust our data, but we want to verify. [LAUGH] The whole point is generalization. And if it’s possible for us to have a little bit of noise in the data, an error here or there, then we want to have some way to deal to handle that possibility, right? I guess so. So, what will we do? [LAUGH] I mean, we actually have a name for this, right? When you get really, really, really good at classifying your training data, but it doesn’t help you to generalize, we have a name for that. Right. That sounds like overfitting. Exactly. We have to worry about overfitting. So you can overfit with the decision tree too? Yeah. What, you don’t believe that? No, no, no. I was, I was being naive, I was being, I know that you can overfit a decision tree. [LAUGH] I was just. [LAUGH] Yeah but your [SOUND] is the [SOUND] that you use when you’re, when you’re like, I don’t believe what you just said Charles, but I’m going to go along with it anyway, because I have to get off the phone soon. [LAUGH] Fair enough. I’ll try to, I’ll try to have a different personality then. [LAUGH] Okay, step one, have a different personality with maximal information gain. Okay, so we don’t want to, we don’t want to overfit. So we need to come up with some way of overfitting. Now the way you overfit in a decision tree is basically by having a tree that’s too big, it’s too complicated. All right. Violates Occam’s Razor. So, what’s a kind of, let’s say, modification to something like ID3 to our decision tree algorithm that will help us to avoid overfitting? Well last time we talked about overfitting, we said cross-validation was a good way of dealing with it, which, it allowed us to choose from among the different, say degrees of the polynomial. Right. So maybe we could do something like that? I don’t know. Try all the different trees and, see which one has the lowest cross validation error? Maybe there’s too many trees. Maybe, but that’s a perfectly reasonable thing to do, right? You take out a validation set. You build a decision tree, and you test it on the, on the validation set and you pick whichever one has the lowest error in the validation sect, that’s one way to avoid it. And then you have, don’t have to worry about this question about stopping, you just grow the tree on the training set minus the validation set until it does well on that. And you check it against the crossvalid, you check it against the validation set, and you pick the best one. That’s one way of doing it, and that would work perfectly fine. There is another way you can do it that’s more efficient. Which is, you do the same idea validation, except that you hold out a set and as you, everytime you decide whether to expand the tree or not, you check to see how this would do so far in the validation set. And if the error is low enough, then you stop expanding the tree. That’s one way of doing it. So is there, is there a problem in terms of, I mean if we’re expanding the tree depth for search wise, we could be at, you know, we could be looking at one tiny little split on one side of the tree before we even look at any, anything on the other side of the tree. That’s a fine point. So how would you fix that? Maybe expand breadth first? Yeah, that would probably do it. Anything else you could think of? Well, so, you could do pruning, right? You could go ahead and do the tree as if you didn’t have to worry about over-fitting, and once you have the full tree built, you could then do a kind of, you could do pruning. You could go to the leaves of the tree and say, well, what if I collapse these leaves back up into the tree? How does that create error on my validation set? And if the error is too big, then you don’t do it. And if it’s very small, then you go ahead and do it. And that should help you with overfitting. So, that whole class of ways of doing it, is called pruning. And there’s a whole bunch of different ways you might prune. But pruning, itself, is one way of dealing with overfitting, and giving you a smaller tree. And it’s a very simple addition to the standard ID3 algorithm.

1.1.29 Decision Trees Other Considerations Regression
So another consideration we might want to think about with decision trees but you’re not going to go into a lot of detail but I think might be worth at least mentioning is the problem of regression. So, so far we’ve only been doing classification ,where the outputs are discreet, but what if we were trying to solve something that looked more like x squared or two x plus 17, or some other continuous function. In other words, a regression problem. How would we have to adapt decision trees, to do that? Any ideas Michael? So these are now continuous outputs, not just continuous inputs. Right, maybe the outputs are all continuous, maybe the outputs are discrete, maybe they’re a mix of both. Well it certainly seems like out rule of using, information gain is going to run into trouble because it’s not really clear how you measure information on these continuous values. So, I guess you could measure error some other way. Well we’re not, it’s not, it’s not error right it’s tryin to measure how mixed up things are? Oh so ,maybe something like variance? Cause in a continuous space you could talk about you know, if there’s a big spread of, in the values that, that would be measured by the variance. Oh good. So what you really have now is a question about splitting. What’s the splitting criteria? Maybe [CROSSTALK] I guess there’s also an issue of, of what you do in the leaves. Right. So, what might you do in the leaves? I guess you could do some sort of more standard kind of fitting algorithm. So, like, report the average or, or do some kind of a linear fit. [SOUND] Is any number of things you can do. By the way ,that’s worth pointing out on the, on the output that if we do pruning like we did before, we have errors, we did actually say when we talked about that how you would report an output. Right? If you don’t have a clear answer where everything is labeled true or everything is labled false, how do you pick? So something like an average would work there. I don’t know, I mean, it seems like it depends on what we’re trying to measure with the tree. If the tree is, we’re trying to get as many right answers as we can, then you probably want to do like a vote in the leaves. Right, which ,at least, if the only answer is true or false, that would look more like an average I guess. Right, so you pick, you do a vote. So we do a vote, so we do pruning. We do have to deal with this issue of the output. Somehow ,and something like a vote mixing. And here, when you have a regression, then I guess average is a lot like voting. Yeah, in a continuous phase. Yeah. So either way we’re doing a kind of voting. I like that.

1.1.30 Decision Trees Wrap up
Hi Michael, so that covers Decision Trees Excellent. So, since you are the one who is listening, you get to tell me what we have learned today? Well, we learned about the Decision Tree representation, we learned the top down algorithm for inducing a Decision Tree. And we call that ID3 All right. So we got a representation, we got a top down learning algorithm ID3. We learned about the, the expressiveness and the bias. Right. So those are 2 separate things, we learned about the sort of expressiveness. And we learned about the bias of ID3. And we gave one, so is this specific to ID3? We, we looked at one specific way of deciding on splits, which was to do this maximum information game. Right. So we talked about in general,um, what are good attributes or what are best attributes. So, information gain is one way of doing it. As one example. And, by the way, this notion of best attribute is something we’ll end up returning to sometimes explicitly, and sometimes implicitly, throughout the entire course. And lastly, I feel like we talked about the problems with over-fitting and how in the Decision Tree context, you can prune back the tree to avoid over-fitting. Over- fitting is an issue. Over-fitting is always an issue and we came up with a couple of strategies for dealing with over-fitting in the context of Decision Trees. Okay! So we’ve learned everything there is to know about Decision Trees, there’s nothing else to know. [LAUGH] Somehow I find that hard to believe. Yeah, there’s a lot there and the students will get a chance to learn even more as they do the assignments. Cool. Excellent. All right Michael, thank you. Sure, look forward to the next chat.

1.1.31 What is Regression Question
Hey Charles, how you doing? I’m doing just fine Michael, how are you doing? I’m doing pretty well, thanks. I’m happy to get a chance to tell you about something today. Excellent, and what is it you’re going to tell me about? We’re going to talk about regression. Like, progression? [LAUGH]. No, regression. So, let me tell you about regression. So we are, in this section of the class, talking about supervised learning. And Supervised learning, in supervised learning we can take examples of inputs and outputs and based on that we are going to be able to take a new input and predict the corresponding output for that input, right. So this, this covers all of this, this the things we are talking about in the context of supervised learning, right. Right. Now, what makes regression special subtopic. We are going to be talking about mapping continuous inputs to outputs. As opposed to, what was the other thing that we were mapping, what other kinds of outputs did we think about? Well, we had discrete outputs and continuous outputs. Right, and so this is going to be the focus on continuous. So regression seems like sort of an odd word. It doesn’t really kind of fit for this. So often I think about regression as. So this is, this is me being all sad and sort of reverting back to a childhood state. And that’s, you know, that’s in the psychological sense, that’s what regression refers to. But it turns out that, that’s not what it means in this setting. But the story by which those things became linked, I think, is kind of interesting. So let me tell you about that. Okay. So, this is a picture of you Charles. [LAUGH] Okay. I’ll accept that. You can tell it’s you because he’s really tall. And you’re, you’re a fairly tall man. I know you don’t think of yourself that way, but you think of everyone else as being short which is really the same thing. Fair enough. Alright, so let’s say that this is Charles. Let’s say that this is someone of average height. Just someone at random. Mm-hmm. So now, let’s pretend, Charles, that you have children. I do have children. All right. So let’s, that’s okay but we can just pretend, and we want to ask the question what would you expect the average height of your children to be? Would you expect it to be sort of, you know, sort of Charles’ height? Or average height or may be somewhere between. So let’s let’s actually ask this as a quiz.

1.1.32 What is Regression Solution
Okay, Charles, so what do you, what do you think about this? Wait, how old are my children? Let’s say what their adult height is going to be. I would expect them to be a little bit smaller than me. A little bit smaller than you? Mm-hmm. So, so their, the average height, their average height of your children, you would you say it would be like an average height person? Or like your height or sort of in between? In between. In between. All right. So it turns out that if you actually do this, you measure people’s heights and you measure the heights of their children, that that is in fact what you tend to see. That very, very tall people, like you tend to have taller than average children. But the height is between. It actually regresses to the mean. And here we really do mean regresses in the sense of going back to this kind of more primitive state that, if you think about average height people, as being like your ancestors, then, you know, you as a, as a very tall person tend to have kids that that tend regress back toward that average value that sort of, more older, more ancient value. So does that that make some sense to you? That makes some sense. But one comment and one question. Comment, that is awesome because I’ve always actually wondered what regresses to the mean actually means. The second, what prevents us from all being the same height then? Yes, so what, what seems to be happening is that there’s a kind of a noisy process and some people turn out to be taller, but then the, then the next generation there’s a little bit of a history effect in people stay taller, but it tends to drift back towards the mean. So it’s, so it’s, it’s sort of like a random walk, to some extent. Oh, that actually kind of makes sense.

1.1.33 Regression and Function Approximation
Alright, so what does this have to do with function approximation or regression. So how does this notion of regression of falling back toward the mean have to do with this idea of approximating functions, map- ping inputs to outputs, it seems kind of odd. So it turns out that the relationship is, here’s the, here’s the connection between them. I’m going to draw a graph and on this axis will be the parent height. And on this axis will be the average child height. So if we plot these against each other, let’s let me put the mean up here. Let’s say that this is mean height for the population. And now say that you know pair, we sort of imagine that parents of average height will have children of average height. But parents that are really tall, like that hypothetical person from before, will have children that are taller than average but not as tall as themselves. And similarly people that are very let’s say of smaller stature will have children that are also you know short. And, but not quite as short again closer to the mean. And it turns out that you have this, this very nice linear relationship between these quantities, and, there’s an important aspect to this. Which is that the slope of this line is less than one, it’s two thirds. Right. If the slope of this line was one, what would that mean Charles? That would mean that everybody’s children had the, would, the same height of their parents. Right, right, and so that’s exactly right, and so but if this slope is less than one, like it is, it turns out to be in, in real populations. Then what’s happening is the children are little shorter than the parents. Children of taller parents are shorter than they are. And the children of short parents are taller than they are. And that’s the fact that this is less than one is what makes it regression to the mean. Now this, this was worked out in I believe in the late 1800s and it was just such a beautiful way of connecting all these different quantities together. To kind of think of them as being related in this functional way. That people said, oh this is really great. I’m going to use this idea of regression. And what they started to mean actually was this not this idea of regression to the mean. But this idea of finding a, a mathematical rela- tionship based on a bunch of measurements of points. So this term ended up getting misused. But that’s the term that we have now. So regression now refers to not this idea of collapsing back towards, towards the mean, but, the idea of using functional form to approximate a bunch of data points. Isn’t that weird. That’s pretty cool. There’s another example of this sort of idea where where a reasonable word, like, like regression which we’re referring to some physical thing in the, in the world due to experiments like psych experiments at this point became this mathematical concept where the name doesn’t really fit anymore, like there isn’t really anything regressing in what we’re doing. Mm-hm. There’s another, really important example that we’re going to get to the later in the course. Do you, do you know what I’m thinking of Charles? No. So reinforcement learning is my field of study. And often your field of study. Often. Often. And it turns out that reinforcement learning doesn’t mean what the words mean anymore. That this was a concept that the psychologist used to explain what they were observing. And then some mathematicians, well let’s call them computer scientists, took the word themselves, started to use it and used it wrong, but now it stuck. [LAUGH] And regression is another example like that. They, the word is sort of being used wrong, but it stuck and that’s what we’re going to use. This explains why every time I have a conversation with a psychologist about reinforcement learning, we talk past each other. Yes, they get very confused. I tried it to tell them upfront that’s not really what I mean, but I’m going to use the words anyway, but it still confuses them. Hmm.

1.1.34 Linear Regression
Alright, so, one of the things that’s very helpful about regression is that in many ways it’s very simple to visualize, it’s very simple to think about what some of the issues are and all the various topics in machine learning that are really important to understand and sometimes are difficult concepts really do come up in a fairly easy to understand way. So what I’d like to do now is to step through an example Of doing some regression and to point out what some of the pitfalls are and how they’re generally handled in the machine learning context. So, this graph that I put up here, is, we just made these numbers up, but it’s supposed to tell us a, a little bit about housing prices. So let’s imagine that we’re off to buy a house and What we notice is that there’s lots of different houses on the market, and there are lots of different, sizes, right. So ,the square footage of the house can vary. And in this case the houses that I visited can be between, about 1,000 to 10,000 square feet. And of course, as you get bigger houses, you tend to get more, the, the prices tend to go up, too. Alright, so the price that the house cost is, tends to rise with the size of the house. So, what I’ve done here is I’ve plotted as a little x say a set of nine houses that I’ve observed. Start off over here with a house that’s a 1,000 square feet and cost a $1,000? I don’t know what year this happened in. And we end up with a house that is 10,000 square feet and cost about $6,000. Again, I don’t. This is not true in Providence Rhode Island, I’ll tell you that. Are you sure? Yeah, I’m pretty sure. Yeah, it’s really not true in Atlanta Georgia. So Alright... So, so, so imagine that this is the relationship we observe. But now we want to answer a question like, Well, what happens If we find a house on the market and it’s about $5,000, what do you think a fair price for that would be? So what do you, what do you think, Charles? Looking at this, what do you think a fair price for a 5,000 square foot house would be? Apparently about $5,000. About, $5,000. Right. So, how did you do that? I looked at the graph, I went over to 5,000 square feet at the x-axis and I went up. Until I found ,where one of the x’s was on the y axis and I said, oh, that’s about 5,000 square feet. Well, but there was no corresponding point for that, so you had to interpolate or something ,uh, based on the points that were there you had to kind of imagine what might, might be happening at the 5,000 square foot mark, right? That’s true, although this one was a little easy because at 4,000 and 6,000 square feet, they were almost exactly the same. Mm, and so that, to you, made it feel like there was probably ,um, that’s probably the level where things in this range would be. Yeah. Okay. Alright, that seems kind of reasonable. So sure, though what we’re going to do in this case is actually try to find a, a function that fits this. Mm-hm. Alright ,so what we can do is actually say, well what if there is a linear relationship. What would be the best linear function that captures the relationship between the size and the cost. So ,what I have here is, it turns out of all the possible linear functions, this is the one that minimizes the squared error, the squared deviation, between these x points and the corresponding position on green line. So it finds a way of balancing all those different errors against each other and that’s the best line we’ve got. Now in this particular case, it’s interesting right, because if you put your idea of 5,000 square feet. Look what this line predicts. It’s something more like $4,000, right. Do you see that? I do. That is doesn’t seem right to me. It doesn’t, yeah, it doesn’t really look like a very good fit. But it does at least capture the fact that there is increasing cost with, with increase in size. That’s true.

1.1.35 Find the Best Fit Question
Alright. So it’s worth asking, how do we find this best line? So again there’s an infinite number of lines. How do we find one that fits these points the best. And again we’re defining best fit. As the one that has the least squared error, where the error is going to be some of the distances between these x points and the green line that we, that we fit. I’m not even sure that this really is the best fit in this case. I just kind of hand drew it. So, okay. So is this something that we, that we would want to solve by hill climbing which is to say, we kind of pick the. The slope and the intercept of the line, and we just kind of try different values of this until it gets better and better and then we can’t get any better, and we stop. Can we do this using calculus? Can we use random search, where we just like, pick a random M, pick a random B, and see if we’re happy with it? Or is this the sort of thing where we probably would just go and ask a physicist because it involves, like, continuous quantities, and we’re discrete people?

1.1.36 Find the Best Fit Solution
And that’s the correct answer. All right. So let’s actually go through that exercise and derive how we do that. because it’s not so bad in two dimensions and it generalizes to higher dimensions as well. Okay. So it turns out that we can use calculus to do this, I am not going to step through the two-variable example for reasons that I am embarrassed to say. But I am going to show you a different example. So imagine that what we’re trying to do is that we’ve got a bunch of data points, and we’re trying to find the best constant function, right? So the best function that has the form, the value of the function for any given X is always the same constant, C. So if our data looks like this, we got a bunch of X’s and a bunch of Y’s, then what we’re going to do, we’re going to say for any given value of C, any given constant, we can have an error. What’s the error going to be? The error is going to be the sum over all of the data points. Speaker 1: The square difference between that constant we chose and what the actual yi value is. So that’s why. Michael. These differences here. Yes, Charles. Can I ask you a question? Sure. Why are we doing sum of squares? There is many different error functions and sometimes called a, a relative concept called the loss function. There is lots of difference once that could work, you can do the absolute error, you can do the squared error, you can do various kinds of squashed errors where you know. The errors count different depending on how, how much deviation there is. It turns out that this one is particularly well behaved because of this reason that I’m explaining now that that because this error function is smooth as a function of the constant C, we can use calculus to actually find the minimum error value. But there’s lots of other things that could work and they actually do find utility in various different machine learning settings. Okay. So just now using the chain rule, if you want to find how do this error function output change as a function of input c. We can take the derivative of this sum you know, bring the two over. Times this, times the derivative of the inside, which is negative one in this case. And now this gives us a nice, smooth function saying what the error is as a function of c. And if we want to find the minimum, what do we want to do to this quantity? Set it equal to zero, because that’s what I remember from Calculus. That’s right. So in particular if the error you know, the error function is a nice smooth thing the derivative is negative and then zero and then positive. When it hits zero that’s when the thing has bottomed-out. Alright. So now we just need to solve this, this equation for c. So we have one equation and one unknown. Alright, so that gets us this. But, this quantity, it’s just the constant added to itself n times. So it’s n times c. We move that to the other side. We get n times c. N is the number of data points as you recall. Is the sum of the yi’s. We divide two by n and what do we see? So what is it Charles? The best constant is the average of all your y’s. Great, it’s the mean. The mean comes back. Right, so in the case of finding the best constant here, we just have to average the y, the y’s together and that catches thing that minimizes the squared air. So squared air is this really nice thing because it tends to bring things like mean back into the picture. It’s really very convenient. And, it generalizes to higher, higher order of function tier, not higher functions, but more variables like, like lines. Sorry. Lines that have some, some non constant slope. By doing the same kind of process and things actually work really nicely.

1.1.37 Order of Polynomial
Alright, so now let’s, let’s get back to our data set that we were looking at before. So again, the ideas that we’re, we’re going to try to find a way of predicting the value for various points along the way on this curve. And one thing we could do is find the best line. But we also talked now about finding the best constant. Turns out these all belong to a family of functions that we could fit. Which are functions of this form. Alright. We’ve got x is our input and what we’re going to do is we’re going to take some constant and add that to some scaled version of x times some scaled version of x squared plus some scaled version of x cubed, all the way up to some order k. And we’ve talked about k equals zero, the constant function. And k equals one, the line. But there’s also k equals two, parabola. Would it probably be a good choice at this particular case? Yes. It does seem like it’s got, sort of, curvy downy nature, right? Mm hm. It’s going up and it’s kind of flattening out and maybe we could imagine that it starts coming down again? At least, over the course of these points, it doesn’t come down again but at least it sort of flattened out. So let’s take a look at that. Let’s take a look at the. The best parabola to fit this. Alright, so, so here we go. We’ve got the, the best line now, the best constant function which is just the average. We have the best line with some slope to it. That’s the green one. We have now the best parabola and look at it, it does, it does a nice job, right? Kind a gets, gets tucked in with all those other points. so, so what do you think? Is this the best way of, of capturing this. This particular set of points? Well, if the only thing we care about is minimizing the sum of squared error, my guess is that the parabola has less squared error. Yeah. It ha, there’s more degrees of freedom so at the worst we could have just fit the parabola as a line. Right. We can always just set any of these coefficients to, to zero. So if the best fit to this really was a line. The best fit to this data point was a line, then the parabola that we see here wouldn’t have any curve to it. So, yeah. Our arrows going down. Hm, As we have gone from order zero to order one to order two. So can you think of any other way getting there in order to getting down even more. How about order 14 million. Interesting, while in this particular case, given the amount of data that we have, we can’t go past the number of data points, yeh after that. They’re really unconstrained. Okay. Then how about order nine? Order nine is a good idea. But just to give you an idea here, we’re going to step up a little more. This is order four and look at, look at how lovely it can actually capture the flow here. That’s, very faded. Order six and in fact the best we can do here is of the, of the, sorry. The most, the highest order that, that works is order eight. And, son of a gun, look what it did. It hit every single point dead on in the center. Boom. Boom. Boom. Boom. It used all the degrees of freedom it had to reduce the error to essentially zero. Excellent. So [LAUGH], one could argue that this is a really good idea. Though, if you look at what happens around 9000, there’s some craziness. Do you see that? I do. At the Yeah, the To try to get this particular parabola to hit that particular point, it sent the curve soaring down with an up again. We also did that between 6500 and 8500, it sent [CROSSTALK]. Yes good point [UNKNOWN] [CROSSTALK] Right, right, that’s right went off the top of the plot. So Yeah, that’s kind of [INAUDIBLE]. But let’s just, just to show that we really are, as we have more degrees of freedom we’re fitting the error better. Let me show you what it looks like, the amount of error for the best fit for each of these orders of k. Alright and so, so what you see when we actually plot the, the squared error, this function that we’re trying to minimize. As we go from order zero to order one, order two, order three, order four, order five, all the way to eight. By eight, there is no error left because it nailed every single point. So you know its kind of a good, nut it doesn’t feel quite right like the curves that we’re looking there looked a little bit crazy.

1.1.38 Pick the Degree Question
All right, so let’s, let’s do a quiz. Give you a chance to kind of think about what where are these trade-offs are actually going to be. So we’re going to pick the degree for the housing data, and your choices are going to be the degree zero, one, two, three, or eight. So a constant, that’s the first choice. Or a line that has some slope that, you know, sort of increases with the data, that’s your second choice. Or it could be we use a degree two parabola. So sort of goes up and then levels off. Or you can it might be a little hard to see but here’s a cubic that that goes up flattens out a little bit and then rises up again at the end. Or we could go with the full monty, the octic. You can see that might not be spelled correctly. That actually has enough degrees of freedom that it can hit each of these points perfectly. Like that. The authority line. [LAUGH] Good, you got that in.

1.1.39 Pick the Degree Solution
So Charles, how would we go about trying to figure this out? How would we go about trying to figure this out? Yeah, what do you think? Which one would you choose and how would you choose? so, well that’s a good question. Well just given what you, what you’ve given me, I’m going to ask. I think smartly guess, that probably k equals 3, is the right one, and K equals 3 And I’ll tell you why. It’s because zero, one and two seem to make quite a few errors. Mh-hm Three does a pretty good job but doesn’t, doesn’t over commit to the data. Hm. And that’s the problem with eight, is that eight says, you know, the training data that I have is exactly right and I should been and moved heaven and earth in order to, to match the data. And that’s probably the wrong thing, certainly if there’s any noise or, or anything else going on in the data. Right. So it sort of seems like it’s overkill, especially that it’s doing these crazy things between the points. Whereas the cubic one, even though it clings pretty close to the points, it stays between the points, kind of between the points. Yeah. Which seems like a really smart thing. So yeah so, so that turns out to be the right answer but let’s actually let’s actually evaluate that more concretely.

1.1.40 Polynomial Regression
Alright. So we talked through how it works when you’ve got you’re trying to fit your data to a constant function, to a zero order polynomial. But let’s, let’s at least talk through how you do this in the more general case. This is, this is what I’ve been doing to, to fit various curves to the data at least implicitly. So, what we’re really trying to do is we’ve got a set of data, x and y. Set n, n examples of x’s and their corresponding y’s. And what we’re trying to find is these coefficients, C0, C1, C2, C3. Let’s say if we’re trying to do cubic regression where C0 gets added to C1 times x, which gets added to C2 times x squared. Which gets added to C3 times X cubed and we’re trying to get that to look a lot like y. Now we’re not going to get to exactly equal y but let’s pretend for a moment that we could. We have a bunch of these examples and we want it to work for all of them. So we can arrange all of the, all these constraints, all these equations into matrix form. If you’re familiar with linear algebra. So the way that we can write this is here are the, here are the coefficients that we’re looking for, the C’s, and here are what we’re going to multiply them by. We’re going to take the X one and look at the zeroth power, the second power, the third power. And that equation I’ll use my hands cause that’s I always, I always need to use my hands when I do matrix multiplication. So you’re going to across here and down there to multiply these and add. And that needs to correspond to y1. And same thing this now the second row. Multiplied by these coefficients. Need to give us our y2 and so forth. Alright. So if we arrange all these x values into a matrix, and we’ll call it, you know, x. And then we have these other guys. And we’ll call this w, like the coefficents. Obviously w stands for coefficent. And we want that to sort of equal This vector of y’s. And we basically just need to solve this equation for the w’s. Now, we can’t exactly solve it because it’s not going to exactly equal, but we can solve it in a least squares sense. So let me just step through the steps for doing that. Alright, so let’s, so here’s how we’re going to solve for w. So what we’re going to do is premultiply by the transpose of x. Both sides. I mean really what we wanted to do at first is if we are solving for Y, we need to multiply by the inverse of X, but this isn’t really going to be necessarily well behaved. But if we pre mulitplied by the X transpose then this thing is going to have a nice inverse. So now we can pre multiply by that inverse. All right. Now, conveniently because this has a nice inverse, the inverses cancel each other. [NOISE] We get that the weights we’re looking for can be derived by taking the x matrix times its own transpose, inverting that, multiplying by x transpose and then multiplying it by the y. And that gives us exactly the coefficients that we need To have done our polynomial regression. And it just, it just so happens that we have some nice properties in terms of these x transpose x. Not only is it invertible, but it does the right thing in terms of minimizing the least squares. It does it as a projection. Now, we’re not going to go through the process by by which we argue that this is true. Does it have something to do with calculus? It most likely has something to do with calculus. And we’ll get back to calculus later. But in this particular case we can, we’re just using projections and linear algebra. And most importantly the, the whole process is just we take the, the data we arrange it into this matrix with whatever sort of powers that we care about. And then we just compute this quantity and we’re good to go. Okay.

1.1.41 Errors Question
Alright, now, part of the reason, we can’t just solve these kinds of problems by solving, a system of linear equations and just being done with it, the reason we have to do these squares is because of the presence of errors. The training data that, that we are given, has errors in it. And it’s not that we’re actually modelling, a function, but ,the thing that we’re seeing is the function plus some, you know, some error term on each piece of data. So, I think, it’s reasonable to, to think about where did these errors come from? So, I don’t know, what do you think ,Charles, why, why is it we’re trying to fit data, that ,has error in it, can’t we just, can’t we just have no errors? [LAUGH] I would like to have no errors. Certainly ,my code, has no errors. [CROSSTALK] well, so let’s see where might errors come from. So, they could come from, sensor error, right? Just ,somehow you’re, you’re getting inputs and you’re getting outputs and that output’s, being read by, some machine or by a camera or by something and you just, there’s just error in the way that you read the data. Just an error in the sensors. Alright, can you think of other ways. I guess, I guess ,in this case you’re imagining that the data came by actually measuring something, with the machine. So that, that makes, a lot of sense. What other ways, can we put together the data? I don’t know I could think of a bunch. I mean the error, well, the errors could come, maliciously. There could be some, something out there, that is trying to give us bad data. Alright, that seems like a possibility, that, when the data set was collected, let’s say that we’re collecting, various, Oh, maybe if I. Oh, this happens, this happens a lot. So, so, if you’re trying to collect data from other Computer Science departments and you’re trying to put together, some kind of collection of, you know, how much do you spend on your. Graduate students ,say,uh, sometimes ,these departments will actually misrepresent the data and give you give you, things that are wrong. Because, they don’t want to tell you the truth, because they’re afraid of what you are going to do. Yeah, I’ve noticed that everyone does that except, for Georgia Tech and Brown University. Yeah, there are highly honest and reputable universities in my experience. Yeah, that’s what I feel. well, another time that you can get data, is if somebody, is, copying stuff down. So, what about sort of the idea of a transcription error. Uh-huh. So we’re just, you know, we’ve copied everything, but, you know, there’s, there’s just some of the, some of the lines that got filled in just got mistyped. Yeah, and you think ,that’s different from sensor error? Well, it’s, it’s maybe a slightly different kind of sensor error, right? So ,sensor errors were actually saying there’s something physical, that’s being measured and there’s just noise in that. Transcription error, is similar except it’s a person. [LAUGH] Mm. Right? The, the there’s a little blips in the person’s head and they can do, it can be a very different kind of error. You can get, like transpositions of digits, maybe instead of ,um, just you know, noise. Okay, how bout, how bout one more? How about ,uh, there’s really, just noise, in the process. So how about that, that we took in input X, but there’s something else going on in the world, that we weren’t measuring, and so the output ,might depend on other things besides, simply ,the input that we’re looking at. So what would be an example of that? So an un-modeled influence, might be, well, if we’re. [CROSSTALK] Let’s look at the housing data. That’s what I, that’s what I was thinking exactly. So ,in the housing data ,we were just trying to relate, the size of the houses, to the price, but, there’s a lot of other things like change of the houses to the price and Location, location. Location and location, right those are three really good reasons, that are not in the particular regression, that we did, that could ,actually influence the prices. So right, that and, you know, the quality of the house and who, who built it, and, you know, the colors, the colors. Even, even, even time of day, or what the interest rates were that morning, versus the, what people thought they might be the next day. Who knows? Right and so all these different things are being considered ,in that particular regression, so we’re just kind of imagining ,that it’s noise, that it’s just having a, a ,uh, bumpy influence on the whole process. Sure. All right. So, so what I’d like you to do is select ,the ones that you think actually are important, the ones that, that, that could actually come up, when you’re using machine learning and regression to solve your problems.

1.1.42 Errors Solution
All right, and if you know, if you were paying attention as we were going through this, these are all very common, and realistic things. So, you know these are all true, these are all sources of error. And this is why we really need to be careful when we fit our data. We don’t want to fit the error itself, we want to just fit the underlying signal. So let’s talk about how we might be able to figure that out. How can we, how can we get a handle on what the underlying function really is apart from the errors and the noise that are, that are in it.

1.1.43 Cross Validation
Alright, so let me try to get to this concept of cross validation. So, imagine that we’ve got our data, this is our training set. We can, again, picture geometrically in the case of regression. And, ultimately what we’re trying to do is find a way of predicting values and then testing them. So, what we imagine is we do some kind of regression and we might want to fit this too a line. And, you know, the line is good, it kind of captures what’s going on and if we apply this to the testing set, maybe it’s going to do a pretty good job. But, if we are, you know, feeling kind of obsessive compulsive about it we might say well in this particular case we didn’t actually track all the ups and downs of the data. So what can we do in terms of if we, if we fit it with the line and the errors not so great. What else could we switch to Charles? We could just use the test. No, sorry. What, what I mean is if we fit, we fit this to a line and we’re sort of not happy with the fact that the line isn’t fitting all of the points exactly. We might want to use ,uh, maybe a higher order polynomial. Oh, I’m sorry, totally misunderstood you. To fit this better. So if we, we can fit this with a higher order polynomial and maybe it’ll hit, all these points much better. You know, so we have this kind of, kind of other shape, and now it’s doing this, it’s making weird predictions in certain places. So, really what we’d like to do is, and what was your suggestion? If we trained on the test set, we would do much better on the test set, wouldn’t we? Yes. But that, that, that’s definitely cheating. Why is cheating? Is there some, why is it cheating? Well, if we exactly fit the error, the, the test set. That’s not a function at all, is it? [LAUGH] If we exactly fit the, the test set, then again that’s not going to generalize to how we use it in the real world. So the goal is always to generalize. The test set is just a stand-in For ,what we don’t know we’re going to see in the future. Yes, very well said. Thank you. Actually that suggests something very important, right, it suggest that ,um, nothing we do, on our training set or even if we cheat and use the test set .Actually makes sense unless we believe that somehow the training set and the test set represent the future. Yes, that’s a very good point, that we are assuming that this data is representative of how the system is ultimately going to be used. In fact, there’s an abbreviation that statisticians like to use. That the data, we really count on the data being independent and identically distributed, Mm-hm. which is to say that all the data that we have collected, it’s all really coming from the same source, so there is no, no sort of weirdness that the training set looks different from testing set looks different from the world but they are all drawn from the same distribution. So would you call that a fundamental assumption of supervised learning? I don’t know that I’d call it a fundamental of supervised learning per se, but it’s a fundamental assumption in a lot of the algorithms that we run, that’s for sure. Fair enough. There’s definitely people who have looked at, well what happens in real data if these assumptions are violated? Are there algorithms that we can apply that still do reasonable things? But the stuff that we’re talking about? Yes, this is absolutely. A fundamental assumption. Alright, but here’s, here’s where I’m trying to get with this stuff. So what we really would like to do, is that we’d like to use a model that’s complex enough to actually model the structure that’s in the data that we’re training on, but no so complex that it’s, it’s matching that so directly that it doesn’t really work well on the test set. But unfortunately we don’t really have the test set to play with because that again, is going to, it’s too much teaching to the test. We need to actually learn the true structure that is going to need to be generalized. So, so how do we find out. How can we, how can we pick a model that is complex enough to model the data while making sure that it hasn’t started to kind of diverege in terms of how it’s going to be applied to the test set. If we don’t have access to the test set, is there something that we can use in the training set that we could have it kind of act like a test set? Well, we could take some of the training data and pretend its a test set and that wouldn’t be cheating because its not really the test set. Excellent. Indeed, right, so there’s nothing magic about the training set all needing to be used to fit the coefficient. It could be that we hold out some of it ,as a kind of make pretend test set, a test test set, a trial test set, a what we’re going to say cross validation set. And it’s going to be a stand in for the actual test data. That we can actually, make use of that doesn’t involve actually using the test data directly which is ultimately going to be cheating. So, this cross validation set is going to be really helpful in figuring out what to do. So. Alright, so here’s how we’re going to do this, this concept of cross validation. We’re going to take our training data, and we’re going to split it into what are called folds. I’m not actually sure why they’re called folds. I don’t know if that’s a sheep reference. Why would it be a sheep reference? I think there’s a sheep-related concept that is called a fold. Like, You know, we’re going to bring you back into the fold. Oh. It’s like the, it’s like the group of sheep. You are just trunk full of knowledge. Alright so what we’re going to do is train on the first three folds, and use the fourth one to, to see how we did. Train on the [LAUGH] second there and fourth fold and check on the first one. And we’re going to we’re going to try all these different combinations leaving out each fold as a kind of a, a fake test set. And then average these errors. The ,uh, the, the goodness of fit. Average them all together, to see how well we’ve done. And, the model class, so like the degree of the polynomial in this case that does the best job, the lowest error, is the one that we’re going to go with. Alright, so if this is a little bit abstract still let me, let me ground this back out in the housing example.

1.1.44 Housing Example Revisited
Alright so here’s how we’re going to look at this. So as you may recall, in this housing example. If we look at different degrees of polynomials and how well they fit the data. Let’s look at the training error. The per example training error. So how far off is it for each of the data points? And as we increase the degree of the polynomial from constant to linear to quadratic and all the way up to, when this case order six, the error’s always falling. As you go up, you have more ability to fit the data, closer and closer and closer, right? because, each of these models is, is nested inside the other. We can always go back. If the zero fits best and I give you six degrees of freedom, you can still fit the zero. So, that’s what happens with the training error, but now let’s use this idea of cross validation to say what if we split the data up into chunks and have each chunk being predicted by the, the rest of the data? Train on the rest of the data, predict on the chunk. Repeat that for all the different chunks and average together. So, so I actually did that. And this is what I got with the cross validation error. So there’s a I don’t know there’s a couple of interesting things to note about this plot. So that we see, we have this red plot that is constantly falling and the blue plot which is the cross validation error starts out a little bit higher than the, the red plot that’s got higher error. So, why do you think that is Charles? Well that makes sense right? because we’re actually training to minimize error. We’re actually trying to minimize error on the training set. So the parts we aren’t looking at, you’re more likely to have some error with. That makes sense if you’d have a little bit more error on the data you haven’t seen. Right, so, good. So, so, in the, on the, this red curve. We’re actually predicting predicting all the different data points using all of those same data points. So it is using all the data to predict that data. This blue point, which is really only a little bit higher in this case, is using, in this particular case I used all but one of the examples to predict the remaining example. But it doesn’t have that example when it’s, when it’s doing its fitting. So it’s really predicting on a new example that it hasn’t seen. And so of course you’d expect it to be a little bit worse. In this particular case, the averages are all pretty much the same so there’s not a big difference. But now, let’s, let’s look at what happens as we start to increase the degree, we’ve got the ability to fit this data better and better and better, and, in fact, down at you know say, three and four, they’re actually pretty close in terms of their ability to, to, to fit these examples. And then what’s great, what’s really interesting is what happens is now we start to give it more, the ability to fit the data closer and closer. And by the time we get up to, to order six polynomial, even though the error on the training set is really low, the error on this, on this cross validation error, the error that you, that you’re measuring by predicting the examples that you haven’t seen, is really high. And this is beautiful this, this inverted u, is, is exactly what you tend to see in these kinds of cases. That the error decreases as you have more power and then it starts to increase as you use too much [LAUGH] of that power. Does that make sense to you? It does make sense, so. The, the problem is that as we give it more and more power we’re able to fit the data. But as it gets more and more and more power it tends to over fit the training data at the expense of future generalization. Right. So that’s exactly how we, we referred to this is this sort of idea that if you don’t give yourself enough degrees of freedom, you don’t give yourself a model class that’s powerful enough you will underfit the data. You won’t be able to model what’s actually going on and there’ll be a lot of error. But if you give yourself too much you can overfit the data. You can actually start to model the error and it generalizes very poorly to unseen examples. And somewhere in between is kind of the goldilocks zone. Where we’re not underfitting, and we’re not overfitting. We’re fitting just right. And that’s the point that we really want to find. We want to find the model that fits the data without overfitting, and not underfitting. So what was the answer on the, housing exam? Well, so, it seems pretty clear in this, in this plot that it’s somewhere, it’s either three or four. It turns out, if you look at the actual numbers, three and four are really close. But three is a little bit lower. So three is actually the thing that fits it the best. And, in fact, if you look at what four does. It fits the data by more or less zeroing out the, the quartic term, right? It doesn’t really use the, this power. Oh, but that’s interesting. So that means it, it barely uses the, the, the extra degree of freedom you give it. But even using it a little bit, it still does worse than generalization. Just a tiny bit worse. Huh. Yup exactly so. That’s actually kind of cool.

1.1.45 Other Input Spaces
Alright. Up to this point I’ve been talking about regression in the context of a scalar input and continuous output. Sorry. Scalar input and continuous input. So basically this x variable. But the truth of the matter is we could actually have vector inputs as well. So what would might, what might be an example of where we might want to use a vector input? A couple of things. One if you look at the housing example, like we said earlier, there are a bunch of features that we weren’t keeping track off. So we could have added some of those. Great yeah, we could include more input features and therefore combine more things to get it. But how would we do that? So let’s say for example, that we have. Two input variables that we think might be relevant for figuring out housing costs. The size, which we’ve been looking at already, But also let’s say the distance to the nearest zoo. We, we think that that’s a really important thing. People like to live close to the zoo and so. But probably not too close to the zoo. [LAUGH] Possibly not too close to the zoo. But let’s let’s imagine that it’s like size, something that actually Or actually, let, let’s do it the other way, let’s sort of imagine that, that, that the further away from the zoo, you are, the better it is. Just like the bigger the size is, the better it is. Mm-hm. So how do we combine these two variables into one in the context of the kinds of function classes that we’ve been talking about? Well, if you think about lines, we can just generalize the planes and hyper planes. Right so, in the case of, of a 1 dimensional input. That 1, 1 dimensional input gets mapped to the cost. But in the case of 2 dimensional inputs, like size and distance to the zoo. We have something that’s more like a plane, combining these two things together in, in the linear fashion to actually predict what the. Cost is going to be. So right, so these, this notion of linear functions generalizes, this notion of polynomial function function generalizes too very, very nicely. All right, there is another kind of input that’s important too, that, let’s think about a slightly different example to help drive the idea home. So let’s imagine we are trying to predict. Credit score, what are some things that we might want to use as features to do that. Do you have a job? I do, actually. [LAUGH] yes. Oh, I am sorry, I am sorry, I misunderstood. So you are asking, you are saying one [UNKNOWN] that could be important for predicting someone’s credit score is just to know do they currently have a job. Right another thing might be well you, you can ask instead how much money they actually, how, how much, how many assets they have. How much money do they have? Credit cards. Great. So, so, right. So things like, what is the value of the assets that, that they own, right? So this is a continuous quantity like we’ve been talking about. But something like do you have a job, yes or no, is a discreet quantity. And one of the nice things about these kinds of regression approaches that we’ve talking about, like polynomial regression, is that we can actually feed in these discrete variables as well. Certainly if they’re, if they’re Boolean variables like, do you have a job or not? It, you can just think of that as being a kind of number that’s just zero or one. No, I don’t have a job. Yes, I have a job. What if it’s something like, you know, how many houses do you own? Hmm. That’s pretty easy because that’s, you could just treat that as a As a quantity, a scalar type quantity. What about Are you. Type of job. Type of job, I like that. How about hair color? So, yeah, how would we do that? If we, if we’re trying to feed it in to some kind of regression type algorithm, it needs to be a number or a vector of numbers, and they can be discrete. So right. So how do we encode this as some kind of a numerical value? Well, we could do something ridiculous like actually write down the RGB value which would make it kind of continuous. Interesting. That seems insane, but you could do that. Or you could just enumerate them and just assign them values one through six in this case. Right, 1, 2, 6 or they could be vectors like, is it red, yes or no? Is it beige, yes or no? Is it brown, yes or no? Have it be a vector and actually for different kinds of discreet quantities like this it can make it different, right? So in particular if we just gave the numbers. Then it’s kind of signalling to the algorithm that blonde is halfway between brown and black, which doesn’t really make sense. We could reorder these. Actually the RGB idea doesn’t seem so bad to me. [UNKNOWN] of course, you have an interesting question of what’s the real RGB value. It implies that somehow interpreting between them Make sense. That’s right, that’s right. It also implies an order right. It implies that the scalar order of RGB is somehow mean something that it’s no different from saying red is one and beige is two. So, if we multiply it, for example, by a positive coefficient then the more RGB you have The better or the worse, right? Hmm. Interesting. Though, in fact what I had in mind here is for RGB, it’s three different hair colors. I thought the g stood for green. There’s, people don’t have green hair, they have gray hair. But I thought the g in RGB stood for green. Yeah it does usually but I’m making a hair joke. [LAUGH] Oh oh. I am sorry. I am glad you explained that. You know Michael. No problem sir. I really, I really like the [UNKNOWN] factor idea. Yeah so I think. I think. I imaging that we are going to return to this issues when we start actually encoding problems as mission learning problems. I think your right. But I think that’s I think that’s said about regression for the time being. I agree.

1.1.46 Conclusion
So, Charles, what did we learn about regression? well, we learned a bunch of interesting historical facts about where the word came from, which I thought was interesting anyway. Oh good. We learned about model selection and overfitting. And underfitting. And fitting in general, cross validation. Talked about, how to do linear and polynomial regression. Yeah. That the best constant, that the best constant in terms of squared error is the mean. Mh-hm These little cocktails for that. Well we also did the same thing for well we talked about the process for how you do it in genreral for any polynomial function. I think that’s everything. Well one more thing, and we talked a little bit about representation, and how to make that work in regression. Great, we talked about input representations and what some of the issues are. There. Yeah. Great, I think that’s, I think that’s a good amount. I think so, too.

1.1.47 Neural Networks
Hey Charles. How’s it goin’? It’s going pretty well Michael. How are things going with you? Good. You know I’m excited to tell you about neural networks today. You may be familiar with neural networks because you have one, in your head. I do? Well, yeah. I mean, you have a network neurons. Like, you know, you know neurons, like brain cells. Let me, let me, I’ll draw you one. Okay. So this is my template drawing, a nerve cell, a neuron. And you can, you know, you’ve got billions and billions of these inside your head. And they have you know, most of them have a pretty similar structure, that there’s the, there’s the kind of the main part of the cell called the cell body. And then there’s this thing called an axon which kind of is like a wire going forward to a set of synapses which are kind of little gaps between this neuron and some other neuron. And what happens is, information spike trains Woo woo! Travel down the axon. When the cell body fires it has an electrical impulse it travels down the, the, the axon and then causes across the synapses excitation to occur on other neurons which themselves can fire. Again by sending out spike trains. And so they’re very much a kind of a computational unit and they’re very, very complicated. To a first approximation, as is often true with first approximations they’re very simple. Sort of by definition of first approximation. So what, what, in the field of artificial neural networks we have kind of a cartoonish version of the neuron and networks of neurons and we actually. Put them together to compute various things. And one of the nice things about the, the way that they’re set up is that they can be tuned or changed so that they fire under different conditions and therefore compute different things. And they can be trained through a learning process. So that’s what we’re going to talk through if you haven’t heard about this before. Okay. So we can replace this sort of detailed version of a neuron with a very abstracted way kind of notion of a neuron. And here’s how it’s going to work. We’re going to have inputs that are kind of you know, think of them as firing rates or the strength of inputs. X1, X2, and X3 in this case. Those are multiplied by weight, w1, w2, w3 correspondingly. And so the weights kind of turn up the gain or the sensitivity of the neuron, this unit, to each of the inputs respectively. Then what we’re going to do is we’re going to sum them up. So we’re going to sum. Over all the inputs. The strength of the input times the weight, and that’s going to be the activation. Then we’re going to ask is that greater than, or equal to the firing threshold. And if it is, then we’re going to say the output is one, and if it’s not, we’re going to say the output is zero. So this is a particular kind of neural net unit called Perceptron. Which is a very sexy name because they had very sexy names in the 50s They did. When this was first developed. Alright? So this, this whole neuron concept gets boiled down to something much simpler, which is just, a linear sum followed by a threshold, thresholding operation, right? So it’s worth kind of thinking, how can we, what sort of things can this, can networks of these kinds of units compute? So, let’s see if we can figure some of those things out.

1.1.48 Artificial Neural Networks Question
Alright ,just to make sure that you understand. let’s let’s think through an example. let’s imagine, that we’ve gotta a neuron. We got one of these perception units. And the input, to it ,is one, zero negative one point five. For the three different, inputs in this case. And the corresponding weights, are half three fifths and one... And the threshold, let’s say is zero, meaning that it should fire, if the weighted sum is above zero, or equal to zero, and otherwise, it should not fire. So, what I’d like you to compute, is based, on these numbers, what the output y would be in this case

1.1.49 Artificial Neural Networks Solution
Alright Charles you want to help us kind of work through this example? Sure. So ,we multiply x1 times w1 so that gives us a half Um-huh. We multiply zero times three fifth which would get a zero. Um-huh. And we multiply minus one point five times one. Which will give us minus three halves. And so, the answers negative. Whatever it is. It is right, so it’s, this was negative ahead, negative one and a half plus a half, so it should be negative one. Right. And, but that’s not the output that we should actually produce, right? That’s the activation. What do we do with the activation? Well we see if the activation is above our threshold fata, which in this case is zero, and it is not So the output should be zero. Good.

1.1.50 How Powerful is a Perceptron Unit
Alright. Well we’d like to try to get an understanding of how powerful one of these perceptron units are. So, what is it that they actually do? So they, they return, in this case either 0 or 1 as a function of a bunch of inputs. So let’s just for simplicity of visualization, let’s just imagine that we’ve got 2 inputs, X1 and X2. So Charles, how could we represent the region in this input space that is going to get an output of 0 versus the region that’s going to get an output of 1. Order the weights. Right. So indeed, the weights matter. So let’s, let’s give some concrete values to these weights. And let’s just say, just making these up that weight 1 is a half, weight 2 is a half, and our threshold data is three quarters. So now what we want to do is again, break up this, this space into where’s it going to return 1 and where’s it going to return 0. Okay, so I think I know how to figure this out. So, there’s kind of an, there’s 2 sort of extreme examples, so let’s take a case where X1 is 0. X1 is 0. Okay, good. So that’s this Y axis, uh-huh. Alright. So if X1 is 0, what value would X2 have to be in order to break a threshold of three quarters? Well, the weight on X2 is a half. Mm-hm. So then, the value of X2 would have to be twice as much as the threshold which in this case is one and a half. Right. So we’re trying to figure out where is it, if X1 is 0, where does X2 need to be so that we’re exactly at the threshold. So that’s going to be. Right. The X2 times the weight, which is half has to exactly equal the threshold which is three quarters. So, if we just solve that out, you get X2 equals 3 halves. So okay that’s this point here. That’s going to be a dividing line. So anywhere above here, what’s it going to return? It will return, it will break the threshold, and so it will return a 1. These are all going to be 1s and then below this these are all going to be 0s. Right. Alright. Well now we have a very, very skinny version of the picture. [LAUGH] Well what else can we do? Well we can do the same thing that we just did except we can swap X2 and X1 because, they have the same weight. So, we could say X2 equal to 0 and figure out what the value of X1 has to be. Good, and that seems like it would be exactly the same algebra, and so we get X1 is 3 halves, gives us at the one and a half point above here are going to be 1s and below here are going to be 0s. Okay, so now we’ve got 2 very narrow windows, but what we notice is that the relationships are all linear here. So solving this linear inequality gets us a picture like this. So this perceptron computes a kind of half plane right? So, so the half of the, the plane that’s above this line, the half plane thatt’s above this line is getting us the 1 answers and below that line is giving us a zero answers. So Michael can we generalize from this, so you’re telling me then that because of the linear relationship drawn out by a perceptron that perceptrons are always going to compute lines. Yeah. Always going to compute, yeah these half planes right. So there’s a dividing line where you’re equal to the threshold and that’s always going to be a linear function and then it’s going to be you know, to the right of it or to the left of it, above it or below it but its always halves at that point. Okay, so perception is a linear function, and it computes hyperplanes. Yeah, which maybe in some sense it doesn’t seem that interesting, but it turns out we’re already in a position to compute something fascinating. So let’s do a quiz.

1.1.51 How Powerful is a Perceptron Unit Quiz Question
So this example that we, you know, created just at random actually is it computes an interesting function. So let’s, let’s focus on just the case where our X1 is in the set zero, one and X2 is in the set zero, one. So those are the only inputs that we care about, combinations of those. What is Y computing here? What is the name of that relationship that function that’s being computed? And so, just as a hint, there’s a, there’s a, there’s a nice short one-word answer to this if you can kind of plug it through and see what it is that it’s computing.

1.1.52 How Powerful is a Perceptron Unit Quiz Solution
Charles, can you figure this out? Yes, I believe I can. So, the first thing to note is that because we’re sticking with just 0 and 1, and not all possible values in between, we’re thinking about a binary function. And the output is also binary. Which makes me think of Boolean functions, where zero represents false and one represents true, which is a common trick in machine learning. Alright, so and let me, let me mark those on the picture here. So we’re talking about the only four combinations are here. And you’re saying in particular. That we’re interpreting these as combinations of true and false. Right False, false true false, false true and true, true. Exactly and if you look at it the only way that you get something above the line is when both are true. And that is called and. Also take conjunction. Right, exactly so, exactly so. So this is, even though we’re, you know we’re setting these numerical values but it actually is, gives us a way of specifying a kind of logic key. Right. So here’s a question for you Michael. Could we do or? That’s a very good question. Or looks a lot like And in this space, it, it seems like it aught to be possible. So let’s let’s do that as a quiz. .

1.1.53 How Powerful is a Perceptron Unit OR Quiz Question
Alright, so we’re going to go in the opposite direction now. And we’re saying, we’re going to tell you what we want y to be, we want y to be the or function. So it should be outputting a one if either x one or x two is one, and otherwise it should output a zero. And what you need to do is fill in numbers for weight one, weight two, and theta so that it has that semantics. Now, just so you know, there is no unique answer here. There’s a whole bunch of answers that will work, but we’re going to check to see that you’ve actually typed in one that, that works.

1.1.54 How Powerful is a Perceptron Unit OR Quiz Solution
Alright Charles, let’s, let’s figure this one out. It turns out, as I said, there’s lots of different ways to make this work, but, what we’re going to do is move that line that we had for conjunction. If we, what we really want to do now is figure out how to move it down ,so that now, these three points, are, in the green zone. They’re going to output, one, because they’re the or, and the only one in the, that’s left in the zero zone in the, in the red zone is the zero, zero case. Right. So, How are we going to be able to do that? Well, since ,we want it to be case that, either, X2 or X1, being one get you above the line, then, we need a threshold and a set of weights ,that put either one of them over. You don’t have to have both of them, you only need one of them. Okay. So, let’s imagine a case where X1 is one and X2 is zero ,then basically, oh, there you’re right, there’s a whole lot of answers, so a weight of 1, for X1, would give you a one. Right? Yes, Huh And so, if we made the threshold 1, that would work. What about weight 2? Well, we do exactly the same thing. So, we set, weight 2 equal to 1. And that means, that in the case where both of them are 0, you get 0 plus 0, which gives you something less than 1. If ,one of them is 1 and the other is 0, you get 1, which gives you right at the threshold. And, if both of them, are, one then you get two, which is still greater than one. Good, alright, that seems like it worked. The other way we could do it, is we can keep the weights at in another way we can do it, is keep the weights where they were before, that just moves this line nice and smoothly down. And then, right? So before, we had a, a threshold of, one and a half. Now we need a threshold of, like, a half ,ought to do it. Yep. Or even less, as long as it’s greater than zero. So, a quarter should work, as well. So, good, so, lots, lots of different ways to do that. And, cool. Can we do not? What’s not of two variables? That’s a good question. Let’s do not of one variable, then. Okay.

1.1.55 How Powerful is a Perceptron Unit NOT Quiz Question
Maybe you should help me finish this picture here. So what we’ve got is X1 is our variable and so we can take on any sort of values. And I marked negative one, zero, and one here. And if we’re doing not, right, then what should the output be for each of these different values of X1? So like if the, if the, if X1 is zero, then we want the output to be. one. One. And if X1 is one, we want the output to be Zero. Zero. All right, so now what we’d like you to do is say okay, what should weight one be and what should theta be so that this, you get, we get this kind of knot behavior.

1.1.56 How Powerful is a Perceptron Unit NOT Quiz Solution
Alright Charles, you were about to say, how we could do this. I think the answer is, simply, that we basically need to flip the, here’s my thinking. We need to flip zero and one, which suggests that either our weight or our threshold needs to be negative. And since we, we The threshold is in above, it’s going to end up being our weight being negative. So, let’s say, if we have a zero, we want to turn that into something above the threshold and if it’s a one, we want it to be below the threshold. So, why don’t we make the weight negative one. Okay. And that, that turn a zero into a zero and it will turn a one into a minus one. Alright. And so, then the threshold just has to be zero. So that would mean that anything, I see, so anything that’s negative will be greater than, zero or negative would be greater than or equal to the threshold. And anything on the other side of that. would be under the threshold. So we get this kind of dividing line at one, so were taking advantage of the fact the equation had a greater than or equal to in it. So, yeah, right, that ought to be a Not. So ,we’ve got And, Or, and Not are all expressible, as perceptron units. So and, or, and not are all expressible as perceptron units. Hey that’s great because if we have AND, OR, and NOT, then we can represent any Boolean function. Well, do we know that? We know that if we combine them together, we combine these perceptron units together Can we, can we express any perceptron, oh sorry, any boolean function that we want using a single perception? So, what do we normally do in this case? So ,what’s the most evil function we can think of? Yes indeed, we’ll when we’re woking on, on decision trees The thing that was so evil was the XOR, the called parity more generally. Right. So, alright. I mean, may, maybe if we can do that, we can do anything. So, let’s, let’s give it a shot.

1.1.57 XOR as Perceptron Network Question
Alright so here’s what we’re going to do. We’re going to try to figure out how to draw sorry, compute XOR as instead of a single perceptron, which we know is impossible, we can do it as a network of perceptron. Just to, to make it easier for you, here’s how we’re going to set it up. That we’re, we’ve x1 and x2 as our inputs We’ve got two units. This first unit is just going to compute and add and we already know how to do that. We’ve already figured out what weights need to be, here and here. And what the threshold needs to be, so that the output will be the and of those two inputs. So, that’s all good. But ,what we don’t know ,eh, what, what, it turns out to be the case, that the second unit, with now three inputs, X1, X2, and the and of X1 and X2, can also be made to, or can be, can be, now, we can set the weights on that, so that the output is going to be X or. So, what we’d like you to do is, figure out how to do that. How do you set this weight - Is the input of X1, this way which is the and input, and this way which is the X2 input, and the threshold. So that ,it’s going to actually compute an X or. And, and just so you know, this is not a trick question. You really can do it this time.

1.1.58 XOR as Perceptron Network Solution
So, okay, so, how we, how we going to solve this? Okay, so, I guess the first thing to do is if you look at the table you have at the bottom, it tells us what the truth tables are for AND and XOR, alright? So, we know that Boolean functions, can all be represented as combinations of and or N not. So, I’m going to recommend you feel out that empty column with OR. So, OR is like that. Right. And you’ll notice, if you look at AND OR and XOR that, OR looks just like XOR except ,at the very last row. In the second, okay good, uh-huh, and in that row. Right, and, AND on the other hand, tells us a one only on the last row. So what, I’m going to suggest that we really want that last node to do in your drawing, is to compute the or of X1 or X2. And produce the right answer, except in the case of the last row, which we only want to turn off when and happens to be true. So ,really what that node is, is computing or minus and. Alright, so how do we make this or minus and? So the way we did or before Well we did it a couple of different ways. But one is we gave weights of one on the two inputs. And then a threshold of one. And that made, ignoring everything else at the moment, this unit will now turn on if either x1 or x2 are on. And otherwise it will stay off. Right. So what’s the worst case? The lowest value that you can get. Is when one of those is one and one of those is zero, which means that the, sum into those will be, in fact, one. Yeah. Right? So, if the AND comes out as being true, it’s going to give us some positive value. So, if we just simply have a negative wait there, that will subtract out. Exactly in the case ,when AND is on. It’s not going to quite give us the answer we want, but it’s a good place to start to think about it. Alright, so like just a negative weight, like negative one. Mm-hmm. Alright. So does that work? Not quite. Alright, and why doesn’t it work? Because if, well certainly when and is off then we really are just getting the or, that’s all good. Yeah. But if both x1 and x2 are both on, then the sum here is going to be two minus the one that we get from the AND which is still one. So, minus one isn’t enough? Minus with both, maybe we can do more than that. Maybe we can do minus two What happens if we do minus two? Then we’ve got ,X1 and X2 if they’re both on, then we get a sum of one minus two plus one or zero. Which is less than our threshold so it will output zero. And in the other two cases, right, when and is off than it just acts like or. So this actually kind of does the right thing. Its actually OR minus kind of and times two. [LAUGH] Right. And there you go. And of course there’s an infinite number of solutions to this.

1.1.59 Perceptron Training
Alright. So in the examples up to this point, we’ve be setting the weights by hand to make various functions happen. And that’s not really that useful in the context of machine learning. We’d really like a system, that given examples, finds weights that map the inputs to the outputs. And we’re going to actually look at two different rules that have been developed for doing exactly that, to figuring out what the weights ought to be from training examples. One is called the the Perceptron Rule, and the other is called gradient descent or the Delta Rule. And the difference between them is the perception rule is going to make use of the threshold outputs, and the, the other mechanism is going to use unthreshold values. Alright so what we need to talk about now is the perception rule for, which is, how to set the weights of a single unit. So that it matches some training set. So we’ve got a training set, which is a bunch of examples of x, these are vectors, and we have y’s which are zeros and ones which are the, the output that we want to hit. And what we want to do is set the, set the weights so that we capture this, this same data set. And we’re going to do that by, modifying the weights over time. Oh, Michiel, what’s the series of dashes over on the left. Oh, sorry, right. I should mention that, so one of the things that we’re going to do here is were going to give a learning rate for the weights W, and not give a learning rule for Theta But we do need to learn the theta. So there’s a, there’s a very convenient trick for actually learning them by just treating it as a, as another kind of weight. So if you think about the way that the, the thresholding function works. We’re taking a linear combination of the W’s and X’s, then we’re comparing it to theta,but if you think about just subtracting theta from both sides, then, in some sense theta just becomes another one of the weights, and we’re just comparing to zero. So what, what I did here was took the actual data, the x’s, and I added what is sometimes called a, a bias unit to it. So basically, the input is one always to that. And the weight corresponding to it is going to correspond to negative theta ultimately. So, just, just again, this just simplifies things so that the threshold can be treated the same as the weights. So from now on, we don’t have to worry about the threshold. It just gets folded into the weights, and all our comparisons are going to be just to zero instead of some, instead of theta. Centric, yeah. It certainly makes the math shorter. So okay, so this is what we’re going to do. We’re going to iterate over this training set, grabbing an x, which includes the bias piece, and the y. Where y is our target X is our input. And what we’re going to do is we’re going to change weight i, the, the, the weight corresponding to the ith unit, by the amount that we’re changing the weight by. So this is sort of a tautology, right. This is truly just saying the amount we’ve changed the weight by is exactly delta W - in other words the amount we’ve changed the weight by. So we need to define that what that weight change is. The weight change is going to be find as falls. We’re going to take the target, the thing that we want the output to be. And compare it to, what the network with the current weight actually spits out. So we compute this, this y hat. This approximate output y. By again summing up the inputs according to the weights and comparing it to zero. That gets us a zero one value.So we’re now comparing that to what the actual value is. So what’s going to happen here, if they are both zero so let’s, let’s look at this. Each of y and y hat can only be zero and one. If they are both zeros then this y minus y hat is zero. If they’re both ones and what does that mean? It means the output should have been zero and the output of our current. Network really was zero, so that’s, that’s kind of good. If they are both ones, it means the output was supposed to be one and our network outputted one, and the difference between them is going to be zero. But in this other case, y minus y hat, if the output was supposed to be zero, but we said one, our network says one, then we get a negative one. If the output was supposed to be one and we said zero, then we get a positive one. Okay, so those are the four cases for what’s happening here. We’re going to take that value multiply it by the current input to that unit i, scale it down by the sort of thing that is going to be cut the learning rate and use that as the the weight update change. So essentially what we are saying is if the output is already correct either both on or both off. Then there’s gong to be no change to the weights. But, if our output is wrong. Let’s say that we are giving a one when we should have been giving a zero. That means our, the total here is too large. And so we need to make it smaller. How are we going to make it smaller? Which ever input XI’s correspond too, very large values, we’re going to move those weights very far in a negative direction. We’re taking this negative one times that value times this, this little learning rate. Alright, the other case is if the output was supposed to one but we’re outputting a zero, that means our total is too small. And what this rule says is increase the weights essentially to try to make the sum bigger. Now, we don’t want to kind of overdo it, and that’s what this learning rate is about. Learning rate basically says we’ll figure out the direction that we want to move things and just take a little step in that direction. We’ll keep repeating over all of the, the input output pairs. So, we’ll have a chance to get in to really build things up, but we’re going to do it a little bit at a time so we don’t overshoot. And that’s the rule. It’s actually extremely simple. Like, you, actually writing this in code is, is quite trivial. And and yet, it does some remarkable things. So let’s imagine for a second that we have a training set that looks like this. It’s in two dimensions, again, so that it’s easy to visualize. That we’ve got. A bunch of positive examples, these green x’s and we’ve got a bunch of negative examples these red x’s, and were trying to learn basically a half plane right? Were trying to learn a half plane that separates the positive from the negative examples. So Charles do you see a, a, half plane that we could put in here that would do the trick? I do. What would it look like? It’s that one. By that one do you mean, this one? Yeah. That’s exactly what I was thinking, Michael. That’s awesome! Yeah, there are isn’t, isn’t a whole lot of flexibility in what the answer is in this case, if we really want to get all greens on one side and all the reds on the other. If there is such a half plane that separates the positive from the negative examples, then we say that the data set is linearly separable, right? That there is a way of separating the positives and negatives with a line. And what’s cool about the perception rule, is that if we have data that is linearly separable. The Perceptron Rule will find it. It only needs a finite number of iterations to find it. In fact, which I guess is really the same as saying that it will actually find it. It won’t eventually get around to getting to something close to it. It will actually find a line, and it will stop saying okay I now have a set of weights that, that do the trick. So that’s happens if the data set is in fact linearly separable and that’s pretty cool. It’s pretty amazing that it can do that, it’s a very simple rule and it just goes through and iterates and, and solves the problem. So. Charles Sened solves the problem. So. I can think of one. What if it is not linearly separable? Hmm, I see. So, if the data is linearlly separable, then the algorithm works, so the algorithm simply needs to only be run when the data is linearlly separable. It’s generally not that easy tell actually, when your data is linearly separable especially, here we have it in two dimensions, if it’s in 50 dimensions, know whether or not there is a setting of those perimeters that makes it linearly separable, not so clear. Well there is one way you could do it. Whats that? You could run this algorithm, and see if it ever stops. I see, yes of course, there’s a problem with that particular scheme, right, which says, well for one thing this algorithm never stops, so wait, we need to, we need to address that. But, but really we should be running this loop here, while, there’s some error so I neglected to say that before. But what you’ll notice is if you continue to run this after the point where it’s getting all the answers right. It found a set of weights that lineally separate the positive and negative instances what will happen is when it gets to this delta w line that y minus y hat will always be zero the weights will never change we’ll go back and update them by adding zero to them repeatedly over and over again. So. If it ever does reach zero error, if it ever does separate the data set then we can just put a little condition in there and tell it to stop filtering So what you are suggesting is that we could run this algorithm and if it stops then we know that it is linearly separable and if it doesn’t stop Then we know that it’s not linearly separable, right? By this guarantee. Sure. The problem is we, we don’t know when finite is done, right? If, if this were like 1,000 iterations, we could run it for 1,000 if it wasn’t done. It’s not done, but all we know at this point is that it’s a finite number of iterations, and so that could be a thousand, 10 thousand, a million, ten million, we don’t know, so we never know when to stop and declare the data set not linearly separable. Hmm, so if we could do that, then we would have solved the halting problem, and we would all have nobel prizes Well, that’s not necessarily the case. But it’s certainly the other direction is true. That if we could solve the halting problem, then we could solve this. Hm. But it could be that this problem might be solvable even without solving the halting problem. Fair enough. Okay.

1.1.60 Gradient Descent
So we are going to need a learning algorithm that is more robust to non-linear separability or linear non- separability. Does that sound right? Non-linear separability. Non linear separability. Non? Yeah think of it. Left parenthesis, linear sep, spreadability left parenthesis. There we go, that’s right, negating the whole phrase, very good. So and, Gradient descent is going to give us an algorithm for doing exactly that. So, what we’re going to do now is think of things this way. So what we did before was we had a summation over all the different input features of the activation on that input feature times the weight, w, for that input feature. And we sum all those up and we get an activation. And then we have our estimated output as whether or not that activation is greater than or equal to zero. So let’s imagine that the output is not thresholded when we’re doing the training, and what we’re going to do instead is try to figure out the weight so that the Not thresholded value is, as close to the target as we can. So this actually kind of brings us back to the regression story. We can define an error metric on the weight vector w. And the form of that’s going to be one half, times the sum over all the data in the dataset, of what the target was supposed to be for that particular example minus what the activation actually was. Right? The activation being the dot product between the weights and the input and we’re going to square that. We’re going to square that error and we want to try to now minimize that. ¿ Hey Michael, can I ask you a question? Sure. Why one half of that? Mm. Yes. It turns out that it turn, in terms of minimizing the error this is just a constant and it doesn’t matter. So why do we stick in a half there? Let’s get back to that. Okay. Just like in the regression case we’re going to fall back to calculus. Right, calculus is going to tell us how we can push around these weights, to try to push this error down. Right, so we would like to know. How does changing the weight change the error, and let’s push the weight in the direction that causes the error to go down. So we’re going to take the partial derivative of the, this error metric with respect to each of the individual weights, so that we’ll know for each weight which way we should push it a little bit to move in the direction of the gradient. So that’s the partial dif, dif, [INAUDIBLE] So that’s the partial derivitive with respect to weight wi, of exactly this error measure. So to take this partial derivitive we just use the chain rule as we always do. And what is it to take the derivitive of something like this, if you have this quantity here. We take the power, move it to the front, keep this thing, and then take the derivitive of this thing. But that, so this now answers your question, Charles. Why do we put a half in there? Because down the line, it’s going to be really convenient that two and the half canceled out. So, it’s just going to mean that our partial derivative is going to look simpler, even though our error measure looked a little bit more complicated. So so what we’re left with then, is exactly what I said, the sum over all these data points of what was inside this. Quantity here times the derivative of that, and here I expanded the a to be, the definition of the a. Now, we need to take the partial derivative with respect to weight w i of this sum that involves a bunch of the ws in it. So, when don’t match the w i, that derivative is going to be zero because the, you know, changing the weight won’t have any impact on it. The only place where this, changing this weight has any impact is at x of i. So that’s what we end up carrying down. This summation disappears. And all that’s left is just the one term that matches the weight that we care about. So this is what we’re left with. Now the derivative of the error with respect to any weight w sub i. Is exactly this, this sum. The sum of the difference between the activation and the target output times the activation on that input unit You know? That looks exactly like, almost exactly like the rule that we use with the rule that we used perceptron before. It does indeed! What’s the difference? Well, actually let’s Let’s write this down. This is now just a derivative, but let’s actually write down what our weight update is going to be because we’re going to take a little step in the direction of this derivative and it’s going to involve a learning rate.

1.1.61 Comparison of Learning Rules
So here’s our update rules what they end up being. The gradient descent rule we just derived says what we want to do is more the weights in the negative direction of the gradient. So if we negate that expression that we had before and take a little step in that direction we get exactly this expression. Multiply the. The input on that weight times the target minus the activation. Whereas in the perceptron case what we were doing is taking that same activation, thresholding it. Like, determining whether it’s positive or negative. Putting in a zero or a one. And putting that in here, that’s what y hat is. So really it’s the same thing except in one case we have done the thresholding and in the other case we have not done the thresholding. But we end up with two different algorithms with two different behaviors. The perceptron has this nice guarantee. A finite convergence, which is a really good thing, but that’s only in the case where we have linear separability. Whereas the gradient descent rule is good because, calculus. [LAUGH]. I guess that’s not really an answer is it. It’s, the gradient descent rule is good because it’s more. Robust. To to data sets that are not linearly separable, but it’s only going to converge in the limit. To a local optimum. Alright is that, is that the story there Charles? As far as I’m concerned.

1.1.62 Comparison of Learning Rules Quiz Question
So once we see these two things next to each other, it kind of raises the question, why, don’t we just use a gradient decent type ,on an error metric that’s defined in terms of y, hat instead of this, the activation a? because y hat ,is the thing, that we really want to match the output. We don’t really want the activation to match the output. There’s no, there’s no need for that. So, it seemed there’s a, bunch of different possible reasons for that. It could be, well we don’t do that, because, it would just be computationally compactable. It’s too, it’s too much work. Another possibility ,would be, well to do the gradient descent, you’d have to be able to take the derivitive and if we use it in this form, it’s not differentiable. So, we can’t take the derivative. Another one is, well sure we can do all that, it’s not intractable and its not, not differentiable. But, if we do that then the weights tend to grow too fast, until you end up getting unstable answers, and then, the last possible choice that we will give you is. You can do that but you can get multiple different answers and the different answers, behave differently and so this is really just to keep it from being in illdefined.

1.1.63 Comparison of Learning Rules Quiz Solution
So why don’t we do gradient descent on y hat? Well there could be many reasons but the main reason is it’s not differentiable. It’s a just discontinuous function. There’s no way to take the derivative at the point where it’s discontinuous. So this this activation thing. The, the change from. Activation to y hat has this big step function jump in it, right, at zero. So once the activation goes positive, actually at zero. It jumps up to one. And before that, it’s, it’s not. So the derivative is basically zero, and then that. Not differentiable, and then zero again. So really, the zero’s not giving us any direction to push, in terms of how to fix the weights. And the undefined part, of course, doesn’t really give us any information either. So this, this algorithm doesn’t really work, if you. Try to take the derivative through this discontinuous function. But it does kind of, you know. What if we made this, more differentiable? Like, what is it that makes this so undifferentiable? It’s this, it’s this really pointy spot, right. So you could imagine a function that was kind of like this, but then instead of the point spot, it kind of smoothed out a bit. Mm, like that. So kind of a softer version of a threshold, which isn’t exactly a threshold. But it leaks this differentiable. Hm. So that would kind of force the algorithm to put its money where its mouth is. Like if that really is the reason, that the problem is non differentiable, fine. We’ll make it differentiable. Now, how do you like it? I don’t know, how do we like it now [LAUGH]? Well, I’ll tell you how much I like it when you show me a function that acts like that.

1.1.64 Sigmoid
Challenge accepted. We’re going to look at a a function called the sigmoid. Sigmoid meaning s-like, right, sig, sigma-ish, sigmoid. So we’re going to define this, this, the sigmoid using the letter. Sigma and it’s going to be applied to the activation just like we were doing before, but instead of thresholding it at zero, what it’s instead going to do is compute this function of a, one over one plus e to the, e to the minus a, and what do we know about this function? Well, it is. Ought to be clear that as the activation gets less and less and less, we’d want it to go to zero, and in fact it does, right. So, as a goes to negative infinity, the negative a goes to infinity. E to the infinity is something really, really big. So it’s one over 1 plus something really big, which is like 1 over something huge, which is almost zero. So, the sigmoid function goes toward, this function that we defined here, goes to zero as the activation goes. To negative infinity, that’s great, that’s just like threshold, and as the activation gets really really large, we’re talking about e to the minus something really large, which is like e to the almost, or like e to the negative infinity which is like almost zero, so one over one plus zero is essentially one. So on the one limit, it go towards zero, and the other limit it goes towards one, and in fact we can just Draw this so you can see what it really looks like you know, minus five and below it’s essentially at zero, and then it makes this kind of gradual, you can see why it’s sigmoid s-shaped curve, then it comes back up to the top and it’s basically at one by the time it get to five. So instead of just an abrupt of transition to zero, we had this gradual transition between negative five and five. And this is great because it’s differentiable, so. What do you think Charles, does this answer your question? It does, I buy that. Alright good so if we have units like this now we can take derivatives which means we can use this gradient decent idea all over the place. So not only is this function differentiable but the derivative itself has a very beautiful form. In particular it turns out... That if you take the derivative of this sigma function, it can be written as the function itself times one minus the function itself. So this is just, this is just really elegant and simple. So, if you have, you know, the sigma function in your code, there’s nothing special that you need for the derivative. You could just compute it this way. So we would, it’s not a bad exercise to go through and do this. Practice your calculus, we just did this together but it’s not that fun to watch. So I would suggest doing it on your own, and if you have any trouble we’ll, we’ll provide additional information for you to, to help you work that out. But when you do it on your own make sure that no one is watching. [LAUGH] Well they can watch, they just probably won’t enjoy it very much. So, so can we say anything about why this form kind of makes sense? So, so what’s neat about this is. As we, as our activation gets very, very negative, then our sigma value gets closer and closer to zero. And if you look at what our derivative is there, it’s something like zero times something like one minus zero, whereas the derivative as you get to very, very large as, that’s like sigma’s going to one. And you get 1 times 1 minus 1 minus 1, so essentially 1 times 0. So you can see the derivatives flatten out for very large and very negative a’s. And when a is like, zero, so what happens when a is like zero? Boy, what does happen when a is like zero? Charles, what happens if we plug zero into this sigma function? You get one half. Is that obvious? Oh, I see, because e to the minus a, that’s zero, so e to the zero is one, one over one plus one, so a half. And then our derivative at that point is a half times a half, or a quarter, so that’s kind of neat. Mm-hm. So so this is really, this, it’s, it’s in a very nice form for being able to work with it. But it’s probably worth saying that. Surely you could use other functions that are different, and there might be good reasons to do that. This one just happens to be a very nice way of dealing with the threshold in question. Yeah and there’s other ways that are also nice. So again, the main properties here are that as activation gets very negative it goes to zero, as activation gets very positive it goes to one, and there’s this smooth transition in between, there’s other ways of making that shape.

1.1.65 Neural Network Sketch
Alright so we’re now in a great position to talk about what the network part of the neural network is about. So now the idea is that we can construct using exactly these kind of sigmoid units, a chain of relationships between the input layer, which are the different components of x, with the output. Y, and the way this is going to happen is, there’s u, other layers of, of units in between. That each one is computing the weighted sum, signoided, of the layer before it. These other layers of units are often referred to as hidden layers, because you can kind of see the inputs, you can see the outputs. This, this other stuff is, is less constrained. Or indirectly constrained. And what’s happening is that each of these units, it’s, it’s running exactly that kind of, you know, take the weights, multiply by the things coming into it, put it through the sigmoid and that’s your activation, that’s your output. So, so what’s cool about this is, in the case where all these are sigmoid units this mapping from input to output. Is differentiable in terms of the weights, and by saying the whole thing is differentiable, what I’m saying is that we can figure out for any given weight in the network how moving it up or down a little bit is going to change the mapping from inputs to outputs. So we can move all those weights in the direction of producing something more like the output that we want. Even though that there’s all these sort of crazy non linearities in between. And so, this leads to an idea called back propagation, which is really just at its heart, a computationally beneficial organization of the chain rule. We’re just computing the derivatives with respect to all the different weights in the network, all in one convenient way, that has, this, this lovely interpretation of having information flowing from the inputs to the outputs. And then error information flowing back from the outputs towards the inputs, and that tells you how to compute all the derivatives. And then, therefore how to make all the weight updates to make, the network produce something more like what you wanted it to produce. So this is where learning is actually taking place, and it’s really neat! You know, this back propagation is referring to the fact that the errors are flowing backwards. Sometimes it is even called error back propagation. Nice, so here’s a question for you Michael. What happens if I replace the sigmoid units with some other function and, and let’s say that function is also different Well, if it’s differentiable, then we can still do this, this basic kind of trick that says we can compute derivatives, and therefore we can move weights around to try to get the network to produce what we want it to produce. Hmm. That’s a big win. Does it still act like a preceptron? Well, even this doesn’t act exactly like a preceptron, right? So it’s really just analogous to a preceptron, because we’re not really doing the hard thresholding, we don’t have guarantees of, of convergence in finite time. In fact, the error function can have many local optima, and what, what we mean by that is this idea that we’re trying to get the, we’re trying to set the weight so that the error is low, but you can get to these situations where none of the weights can really change without making the error worse. And you’d like to think, well good, then we’re done, we’ve made the error as low as we can make it, but in fact it could actually just be stuck in a local optima, that there’s a much better way of setting the weights It’s just we have to change more than just one weight at a time to get there. Oh so that makes sense, so if we think about sigmoid the sigmoid and the error function that we picked right. The error function was sum of squared airs, so that looks like a porabola in some high dimensional space, but once we start combining them with others like this over, over, and over again Then we have an error space where there may be lots of places that look low but only look low if you’re standing there but globally would not be the lowest point. Right, exactly right and so you can get these situations in just the one unit version where the error function as you said is this nice little parabola and you can move down the gradient and when you get down to the bottom you’re done. But now when we start throwing these networks of units together we can get an error surface that looks just in its cartoon form looks crazy like this, that there’s, it’s smooth but there’s these Place where it goes down, comes up again and goes down maybe further, comes up again and doesn’t come down as far and you could easily get yourself stuck at a point like this where you’re not at the global minimum. Your at some local optimum.

1.1.66 Optimizing Weights
one of things that goes wrong when you try to actually run gradient descent on a complex network with a lot of data is that you can get stuck in these local minima and then you start to wonder boy is there some other way that I can optimize these weights i’m trying to find a set of weights for the neural network that well that what that that tries to minimize error on the training set and so gradient descent is one way to do it and it can get stuck but there’s other kinds of advanced optimization methods have become very appropriate here in fact there’s a lot of people in machine learning who think of optimization and learning is kind of being the same thing that what you’re really trying to do in any kind of learning problem is solved this this high-order very difficult optimization problem to figure out what the learned representation needs to be so i just want to mention in passing so various kinds of advanced methods that that people brought to bear there’s things like mom using momentum terms in the gradient which basically where the idea in the momentum is as we’re doing gradient descents imagine this is our error surface we don’t want to get stuck in this little Bowl here we want to kind of pass all the way through to get to this bowl so maybe we need to just you know continue in the direction we’ve been going so instead of you know think of it as a kind of physical analogy instead of just just going to the bottom of this hill and getting stuck it can kind of bounced out and pop over and come to what might be a lower minima later there’s a lot of work in using higher order derivatives to to better optimize things instead of just thinking about the way that individual weights change the error function to look at combinations of weights Hamiltonians and whatnot there’s various ideas from randomized optimization which were going to get to in a sister course that can be applied to two to make things more robust and sometimes it’s worth thinking you know what we don’t really want to just minimize the error on the training set we may actually want to have some kind of penalty for using using a structure that’s too complex missed when do we when do we see something like this before Charles when we were doing regression and we were talking about overfitting what’s a more or less complex network well there’s two things you can do with network you can add more and more nodes and you can add more and more layers good so right so if we do more nodes that we put into network the more complicated the mapping becomes from input to output the more local minimum we get the more we get they have the ability to actually model the noise which brings up exactly the same overfitting issues it turns out there’s another one that’s actually really interesting in the neural net setting which I think didn’t occur to people in the early days but it became clear and clear over time which is that you can also have a complex network just because the numbers the weights are very large so same number of weight same number of nodes same number of layers but larger numbers often leads to more complex network and the possibility of overfitting and so sometimes we want to penalize the network not just by giving it $PERCENT fewer nodes or layers but also by keeping the numbers in a reasonable range set that makes sense that makes perfect sense

1.1.67 Restriction Bias
So this brings up the issue of what neural nets are more or less appropriate for. What is the restriction bias, and the inductive bias of this class of classifiers, and regression algorithms? So Charles, can you remind us what restriction bias is? Well, restriction bias Tells you something about the representational power of whatever data structure it is that you’re using. So in this case the network of neurons. And it tells you the set of hypotheses that you’re willing to consider. Right, so if, if the, if there’s a great deal of restriction, then there’s lots and lots of different kinds of models that we’re just not even considering. We’re, we’re restricting our view to just a subset of those. So In the case of neural nets, what restrictions are we putting? Well, we started out with a simple perceptron unit, and that we decided was linear. So we were only considering planes. Then we move to networks, so that we could do things like exor, and that allowed us to do more. Then we started sticking Sigmoids and other arbitrary functions and to nodes so that we could represent more and more, and you mention that if you let weights get big and we have lots of layers and lots of nodes, we can be really, really complex. So, it seems to me that we are actually not doing much of a restriction at all. So let me ask you this then Michael. What kind of functions can we represent, clearly we can represent boolean functions, cause we did that. Can we represent continuous functions? That’s, that’s a great question to ask, that’s what we should try to figure that out. So, in the case, as you said, Boolean functions, we can. If we give ourselves a complex enough network with enough units, we can basically map all the different sub components of any Boolean expression to threshold like units and basically build a circuit that can compute whatever Boolean function we want. So that one definitely can happen. So what about continuous functions? So what is it? What is a continuous function? A continuous function is one where, as the input changes the output changes somewhat smoothly, right? There’s no jumps in the function like that. Well, there’s no discon, there’s no discontinuities, that’s for sure. Alright, now if we’ve got a continuous function that we’re trying to model with a neural network. As long as it’s connected, it has no, no discontinuous jumps to any place in the space, we can do this with just a single hidden layer. As long as we have enough hidden units, as long as there’s enough units in that layer. And, essentially one way to think about that is, if we have enough hidden units, each hidden unit can worry about one little patch of the function that, that it needs to model. And they, the patches get set at the hidden. And at the output layer they get stitched together. And if you just have that one layer you can make any function as long as it’s continuous. If it’s Arbitrary. We can still represent that in our neural network. Any mapping from inputs to outputs we can represent, even if it’s discontinuous, just by adding one more hidden layer, so two total hidden layers. And that gives us the ability to not just stitch these patches at their seams, but also to have big jumps between the patches. So in fact, neural networks are not very restrictive in terms of their bias as long as you have a sufficiently complex network structure, right, so maybe multiple hidden layers and multiple units. So that worries me a little bit Michael, because it means that we’re almost certainly going to overfit, right? We’re going to have arbitrarily complicated neural networks and we can represent anything we want to. Including all of the noise that’s represented in our training set. So, how are we going to avoid doing that? Excellent question. So, it seems like there’s, there is exactly that worry. But, it is the case though, that when we train neural networks, we typically give them some bounded number of hidden units and we give them some bounded number of layers. And so, it’s not like any fixed network can actually capture any arbitrary function. So any fixed network can only capture whatever it can capture, which is a smaller set. So going to neural nets in general doesn’t have much restriction. but any given network architecture actually does have a bit more restriction. So that’s one thing, the other is hey, well we can do with overfitting what we’ve done the other times we’ve had to deal with overfitting. And that’s to use ideas like, cross validation. And we used cross validation to decide. How many hidden layers to use. We can use it to decide how many nodes to put in each layer. And we can also use it to decide when to stop training because the weights have gotten too large. So, and this is, it’s probably worth pointing this out that this is kind of a different, different property from the. Other classes of supervised learning algorithms we’ve looked at so far. So in a decision tree, you build up the decision tree, an you may have over fit, but it is what it is. In regression, you know, you solve the regression problem, and again that may have over fit. What’s interesting about neural network training is it’s this iterative process that you started out running, and as it’s running, it’s actually Errors going down and down and down. So, in this standard kind of graph, we get the error on the training set dropping as we increase iterations. It’s doing a better and better job of modeling the training data. But, in classic style, if you look at the error in the, in some kind of held-out test set, or maybe in a cross validation set, you see the error starting out kind of high and maybe dropping along with this, and at some point It actually turns around and goes the other way. So here, even though we’re not changing the network structure itself, we’re just continuing to improve our fit, we actually get this, this pattern that we’ve seen before, that the cross validation error can turn around and, and at this, you know, at this low point, you might have, you might want to just stop training your network there. The more you train it, possibly the worse you’ll do. And again, that, it’s reflecting this idea that the complexity of the network is not just in the nodes and the layers, but also in the magnetude of the weights. Typically what happens in this turnaround point is that some weights are actually getting larger and larger and larger. So, just wanted to highlight that difference between neural net function approximation of what we see in some of the other algorithms

1.1.68 Preference Bias
Alright, you know the issue that we want to make sure that we think about each time we introduce a new kind of supervised learning representation is to ask what its preference bias is. So Charles, can you remind us what preference bias is? Mike researcher bias tells you what it is you are able to represent. Preference bias tells you something about the algorithm that you are using to learn. That tells you, given two representations, why I would prefer one over the other. So, perhaps you think back what we talked about with decision trees, we preferred trees where nodes near the top had high information gain We preferred correct trees. We preferred trees that were shorter to ones that were longer unnecessarily and so on and so forth. So that actually brings up a point here which is, we haven’t actually chosen an algorithm. We talked about how derivatives work, how back propagation works, but you missed telling me one very important thing, which is how do we start? You tell me how to update the weights but, how do I start out with the weights? Do they all start at zero? Do they all start out at one? How do you usually set the weights in the beginning? Yes indeed. We did not talk about that, that’s, it’s really important. You can’t run this algorithm without initializing the weights to something. Right? We did talk about how you update the weights but they don’t just you know, just start undefined and you, you can’t just update something that’s undefined. So we have to set the initial weights to something. So pretty typical thing for people to do, is small, random, values. So why do you suppose we want random values? Because we have no particular reason to pick one set of values over another. So you start somewhere in the space. Probably helps us to avoid local minimum. Yea kind of. I mean there’s also the issue of Well if we run the algorithm multiple times if we get stuck, we like it not to get stuck exactly there again, if do, if you run it again. So it gives some variability, which is a helpful thing in avoiding local minimal. And what do you suppose, it’s important to start with small values. Well you just said. In our discussion before that if the weights get really big that can sometimes lead to over fitting, because it let’s you represent arbitrarily complex functions. Good. And so, and what is that tell us about what the preference bias is then? Well if we start out with small random values. That means we are starting out with low complexity. So that means we prefer Simpler explanations to more complex explanations. And of course the usual stuff like we prefer, correct answers to incorrect answers, and so on and so forth. ¿ So, you’d say that neural-nets implement, or maybe we should say, that neural networks implement a kind of bias that says Prefer correct over incorrect but all things being equal, the simpler explanation, is preferred. Well, if you have the right algorithm. If the algorithm starts with small, random values and tries to stop, you know, when you start over-fitting Then you, cause you’re going to start out with the simpler explanations first before you allow your weights to grow. so you, about that. So this reminiscent of the principal that is known as Occan’s razor which is often stated as entities should not be multiplied unnecessarily. And given that we’re working with neural networks, there’s a lot of unnecessary multiplication that happens. [LAUGH] But, in fact, this actually is referring to exactly what we’ve been talking about. So this unnecessarily is, one interpretation of this is that, ”Well, when is it necessary?” It’s necessary if you’re getting better explanatory power, you’re fitting your data better. So Unnecessarily would mean, well we’re not doing any better at fitting the data. If we’re not doing any better at fitting the data, then we should not multiply entities. And multiply here means make more complex. So don’t make something more complex unless you’re getting better error, or if two things have similar error Choose the simpler one, use the one that’s less complex. That has been shown to, if you mathematize this and you use it in the context of supervised learning, that we’re going to get better generalization error with simpler hypotheses.

1.1.69 Summary
So that brings us to the end of the topics we are going to talk about in terms of neural nets. There’s going to be some interesting stuff for you to do in terms of the homework where you’ll be exposed to some other important concepts. But that’s, that’s all we’re going to lecture about for now. So let’s just remind our- selves what exactly we covered in the neural net section. So Charles what do you remember? I remember perceptrons. I remember. And perceptron was a threshold unit, a linear threshold unit, and we could put networks of them together. Yes. To produce any Boolean function. What else? Oh, we had a learning rule for perceptrons. Mm-hm. Which runs in finite time for linearly separable data sets. And we learned a general differentiable rule. Adding general we learned about propagation using a gradient set. And we talked a little bit about the, about the preference and restriction by c’s of neural networks. Alright, til next time. See you Michael.
